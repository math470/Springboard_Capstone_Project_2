{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "\n",
    "# Yelp Recommender System Project\n",
    "## Part 3 Recommender System (In progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "1. <a href='#1'>Preparation for Recommender systems</a>\n",
    "1. <a href='#2'>Basic Algorithms</a>\n",
    "1. <a href='#3'>Collaborative Filtering</a>\n",
    "1. <a href='#4'>Content-based Filtering</a> \n",
    "\n",
    "_The datasets were cleaned and explored in the data wrangling and EDA parts of this project (see Data wrangling and EDA notebooks for details). Note that the datasets used here contain only __food and restaurant__ related businesses, users, and reviews._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Surprise \n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "\n",
    "from surprise import NormalPredictor\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import CoClustering\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV as GridSearchCV_skl\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### Preparation for Recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews_business_user_info.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>city_state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price_range</th>\n",
       "      <th>business_review_count</th>\n",
       "      <th>categories</th>\n",
       "      <th>user_name</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>user_review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x7mDIiDB3jEiPGPHOmDzyw</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "      <td>iCQpiavjjPzJ5_3gPD5Ebg</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-02-25</td>\n",
       "      <td>Secret Pizza</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.109837</td>\n",
       "      <td>-115.174212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4078</td>\n",
       "      <td>['Pizza', 'Restaurants']</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>2011-02-24</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dDl8zu1vWPdKGihJrwQbpw</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "      <td>pomGBqfbxcqPv14c3XH-ZQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-11-13</td>\n",
       "      <td>Leticia's Mexican Cocina</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.298875</td>\n",
       "      <td>-115.280088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1125</td>\n",
       "      <td>['Restaurants', 'Mexican', 'Bars', 'Nightlife']</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>2011-02-24</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZp4UX5zK3e-c5ZGSeo3kA</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "      <td>jtQARsP6P-LbkyjbO1qNGg</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-23</td>\n",
       "      <td>H&amp;H BBQ Plus 2</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.241809</td>\n",
       "      <td>-115.234495</td>\n",
       "      <td>2.0</td>\n",
       "      <td>115</td>\n",
       "      <td>['American (New)', 'Barbeque', 'Restaurants']</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>2011-02-24</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  x7mDIiDB3jEiPGPHOmDzyw  msQe1u7Z_XuqjGoqhB0J5g  iCQpiavjjPzJ5_3gPD5Ebg   \n",
       "1  dDl8zu1vWPdKGihJrwQbpw  msQe1u7Z_XuqjGoqhB0J5g  pomGBqfbxcqPv14c3XH-ZQ   \n",
       "2  LZp4UX5zK3e-c5ZGSeo3kA  msQe1u7Z_XuqjGoqhB0J5g  jtQARsP6P-LbkyjbO1qNGg   \n",
       "\n",
       "   stars        date             business_name       city state  \\\n",
       "0      2  2011-02-25              Secret Pizza  Las Vegas    NV   \n",
       "1      5  2012-11-13  Leticia's Mexican Cocina  Las Vegas    NV   \n",
       "2      1  2014-10-23            H&H BBQ Plus 2  Las Vegas    NV   \n",
       "\n",
       "      city_state   latitude   longitude  price_range  business_review_count  \\\n",
       "0  Las Vegas, NV  36.109837 -115.174212          1.0                   4078   \n",
       "1  Las Vegas, NV  36.298875 -115.280088          2.0                   1125   \n",
       "2  Las Vegas, NV  36.241809 -115.234495          2.0                    115   \n",
       "\n",
       "                                        categories user_name yelping_since  \\\n",
       "0                         ['Pizza', 'Restaurants']   Melissa    2011-02-24   \n",
       "1  ['Restaurants', 'Mexican', 'Bars', 'Nightlife']   Melissa    2011-02-24   \n",
       "2    ['American (New)', 'Barbeque', 'Restaurants']   Melissa    2011-02-24   \n",
       "\n",
       "   user_review_count  \n",
       "0                 11  \n",
       "1                 11  \n",
       "2                 11  "
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4017884 entries, 0 to 4017883\n",
      "Data columns (total 17 columns):\n",
      "review_id                object\n",
      "user_id                  object\n",
      "business_id              object\n",
      "stars                    int64\n",
      "date                     object\n",
      "business_name            object\n",
      "city                     object\n",
      "state                    object\n",
      "city_state               object\n",
      "latitude                 float64\n",
      "longitude                float64\n",
      "price_range              float64\n",
      "business_review_count    int64\n",
      "categories               object\n",
      "user_name                object\n",
      "yelping_since            object\n",
      "user_review_count        int64\n",
      "dtypes: float64(3), int64(3), object(11)\n",
      "memory usage: 551.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting one metropolitan area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a recommender system, I will use food and restaurant businesses only in one metropolitan area for several reasons. First of all, people normally want to have recommendations in some areas they plan to visit. Moreover, if all cities are used, the matrix by users (rows) and businesses (columns) becomes very sparse and this makes it hard to predict stars.  \n",
    "\n",
    "The city with the most number of reviews was Las Vegas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Las Vegas, NV          1139203\n",
       "Phoenix, AZ             408423\n",
       "Toronto, ON             380025\n",
       "Scottsdale, AZ          217414\n",
       "Charlotte, NC           198690\n",
       "Pittsburgh, PA          157270\n",
       "MontrÃ©al, QC           115208\n",
       "Tempe, AZ               115027\n",
       "Henderson, NV           109831\n",
       "Mesa, AZ                 88576\n",
       "Chandler, AZ             86319\n",
       "Cleveland, OH            81433\n",
       "Madison, WI              71896\n",
       "Gilbert, AZ              68186\n",
       "Calgary, AB              63940\n",
       "Glendale, AZ             53566\n",
       "Mississauga, ON          39518\n",
       "Markham, ON              39319\n",
       "Peoria, AZ               29418\n",
       "North Las Vegas, NV      25355\n",
       "Name: city_state, dtype: int64"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.city_state.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed some of the other cities above are also in the [Las Vegas–Henderson–Paradise, NV  metropolitan area](https://en.wikipedia.org/wiki/Las_Vegas%E2%80%93Henderson%E2%80%93Paradise,_NV_Metropolitan_Statistical_Area). The cities belong to the metropolitan area are Henderson,  North Las Vegas, Paradise, Las Vegas, and Boulder City and I will include reviews in all of these cities. \n",
    "\n",
    "In the EDA part, I already transformed 5 kinds of strings representing Las Vegas into 'Las Vegas'. 'Henderson and Las vegas' actually  representing Henderson was also fixed. Here, I will further clean up strings for North Las Vegas. I did not find any multiple strings for Paradise and Boulder City. I will also check whether there are any same name cities in other states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixing city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Las Vegas, NV    1139203\n",
       "Name: city_state, dtype: int64"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.city=='Las Vegas'].city_state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Paradise, NV    110\n",
       "Name: city_state, dtype: int64"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.city=='Paradise'].city_state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boulder City, NV    5746\n",
       "Name: city_state, dtype: int64"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.city=='Boulder City'].city_state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Henderson, NV    109831\n",
       "Henderson, VA         3\n",
       "Name: city_state, dtype: int64"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.city=='Henderson'].city_state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is Henderson in Virginia! Thus, I need to use the city_state column I made to select the 5 cities in my interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "North Las Vegas, NV    25355\n",
       "N. Las Vegas, NV         287\n",
       "N Las Vegas, NV          113\n",
       "Name: city_state, dtype: int64"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.city.isin(['N Las Vegas', 'N. Las Vegas', 'North Las Vegas'])].city_state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are altogether 25755 and 'N Las Vegas' and 'N. Las Vegas'(400 of them) will be fixed to 'North Las Vegas'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from the EDA part\n",
    "def unify_city_names(df, col_name, possible_names, correct_name):\n",
    "    '''\n",
    "    This function correct all possible city names to a correct name\n",
    "    '''\n",
    "    correct_dict = dict(zip(possible_names,[correct_name]*len(possible_names)))\n",
    "    print(correct_dict)\n",
    "    df[col_name]=df[col_name].replace(correct_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N Las Vegas': 'North Las Vegas', 'N. Las Vegas': 'North Las Vegas'}\n"
     ]
    }
   ],
   "source": [
    "unify_city_names(df, 'city', ['N Las Vegas', 'N. Las Vegas'], 'North Las Vegas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.city.isin(['N Las Vegas', 'N. Las Vegas'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25755"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.city=='North Las Vegas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number looks correct! I also need to fix the city_state column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25355"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.city_state=='North Las Vegas, NV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.city_state.isin(['N Las Vegas, NV', 'N. Las Vegas, NV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N Las Vegas, NV': 'North Las Vegas, NV', 'N. Las Vegas, NV': 'North Las Vegas, NV'}\n"
     ]
    }
   ],
   "source": [
    "unify_city_names(df, 'city_state', ['N Las Vegas, NV', 'N. Las Vegas, NV'], 'North Las Vegas, NV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.city_state.isin(['N Las Vegas, NV', 'N. Las Vegas, NV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25755"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.city_state=='North Las Vegas, NV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting reviews in those cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_states_Vegas = ['Las Vegas, NV','North Las Vegas, NV','Paradise, NV',\n",
    "                    'Boulder City, NV', 'Henderson, NV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280645"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1139203+110+5746+109831+25755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280645"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas = df[df.city_state.isin(city_states_Vegas)]\n",
    "len(df_Vegas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_Vegas) == 1139203+110+5746+109831+25755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of reviews, 1,280,645, looks alright!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Las Vegas, NV', 'Henderson, NV', 'Boulder City, NV',\n",
       "       'North Las Vegas, NV', 'Paradise, NV'], dtype=object)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas.city_state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422409"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of users\n",
    "df_Vegas.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9674"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of businesses\n",
    "df_Vegas.business_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Las Vegas reviews have \n",
    "- 1,280,645 reviews\n",
    "- 422,409 users  \n",
    "- 9,674 businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Vegas reviews\n",
    "#df_Vegas.to_csv('reviews_business_user_info_Vegas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting businesses and users with enough reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_with_enough_reviews(df, column, threshold):\n",
    "    review_counts = df[column].value_counts() \n",
    "    return review_counts[review_counts >= threshold].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20340"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_Vegas.user_id.value_counts()>=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_10more = ids_with_enough_reviews(df_Vegas, 'user_id', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20340"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas[df_Vegas.user_id.isin(user_ids_10more)].user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6268"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_Vegas.business_id.value_counts()>=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_ids_20more = ids_with_enough_reviews(df_Vegas, 'business_id', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6268"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas[df_Vegas.business_id.isin(business_ids_20more)].business_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493658"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_Vegas[(df_Vegas.business_id.isin(business_ids_20more))&(df_Vegas.user_id.isin(user_ids_10more))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Vegas_over_10_20 = df_Vegas[(df_Vegas.business_id.isin(business_ids_20more))&(df_Vegas.user_id.isin(user_ids_10more))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20340, 6266, 493658)"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas_over_10_20.user_id.nunique(), df_Vegas_over_10_20.business_id.nunique(), len(df_Vegas_over_10_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20340, 6266, 493658)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas_over_10_20.user_id.nunique(), df_Vegas_over_10_20.business_id.nunique(), len(df_Vegas_over_10_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing users with less than 10 reviews and businesses with less than 20 reviews, the dataset has\n",
    "- 493,658 reviews\n",
    "- 20,340 users\n",
    "- 6,266 businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reviews left can have users with less than 10 reviews and businesses less than 20 reviews since removing users can further reduce businesses and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Vegas_over_10_20.to_csv('reviews_business_user_info_Vegas_over_10_20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build recommender systems!\n",
    "\n",
    "First, I am going to use the recommender system package called [Surprise](https://surprise.readthedocs.io/en/stable/index.html) for basic and collaborative filtering algorithms. Then, I will build content-based algorithms using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>city_state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price_range</th>\n",
       "      <th>business_review_count</th>\n",
       "      <th>categories</th>\n",
       "      <th>user_name</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>user_review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3528833</th>\n",
       "      <td>cpxD9Yxtv_xu8lf-q38dJA</td>\n",
       "      <td>U4INQZOPSUaj8hMjLlZ3KA</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.112827</td>\n",
       "      <td>-115.172581</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7968</td>\n",
       "      <td>['Steakhouses', 'Breakfast &amp; Brunch', 'Restaur...</td>\n",
       "      <td>Michael</td>\n",
       "      <td>2008-01-31</td>\n",
       "      <td>4182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "3528833  cpxD9Yxtv_xu8lf-q38dJA  U4INQZOPSUaj8hMjLlZ3KA   \n",
       "\n",
       "                    business_id  stars        date business_name       city  \\\n",
       "3528833  4JNXUYY8wbaaDmk3BPzlWw      3  2009-10-12  Mon Ami Gabi  Las Vegas   \n",
       "\n",
       "        state     city_state   latitude   longitude  price_range  \\\n",
       "3528833    NV  Las Vegas, NV  36.112827 -115.172581          2.0   \n",
       "\n",
       "         business_review_count  \\\n",
       "3528833                   7968   \n",
       "\n",
       "                                                categories user_name  \\\n",
       "3528833  ['Steakhouses', 'Breakfast & Brunch', 'Restaur...   Michael   \n",
       "\n",
       "        yelping_since  user_review_count  \n",
       "3528833    2008-01-31               4182  "
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas_over_10_20[(df_Vegas_over_10_20.user_id=='U4INQZOPSUaj8hMjLlZ3KA')&(df_Vegas_over_10_20.business_id=='4JNXUYY8wbaaDmk3BPzlWw')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_Vegas_over_10_20[['user_id', 'business_id', 'stars']], reader)\n",
    "raw_ratings = data.raw_ratings\n",
    "\n",
    "# shuffle ratings \n",
    "random.Random(32).shuffle(raw_ratings)\n",
    "\n",
    "# 90% training and 10% test data\n",
    "threshold = int(.9 * len(raw_ratings))\n",
    "train_raw_ratings = raw_ratings[:threshold]\n",
    "test_raw_ratings = raw_ratings[threshold:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U4INQZOPSUaj8hMjLlZ3KA', '4JNXUYY8wbaaDmk3BPzlWw', 3.0, None),\n",
       " ('cg2P244yON3-_GXWkgAgsw', 'umXvdus9LbC6oxtLdXelFQ', 4.0, None),\n",
       " ('YHdXkAmndIfuIczWOnsjeQ', '7HIa2lYy5jgcZuADlRjKSg', 1.0, None),\n",
       " ('CstEf6M4JSom9Msm0qIYew', 'wdOOK3K6vzQy1d_OIk-U9w', 3.0, None),\n",
       " ('Cwkkowhq9MZue1Xyk57BMg', '7sPNbCx7vGAaH7SbNPZ6oA', 4.0, None)]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ratings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm(train_set, test_set, algorithm, param_grid, n_cv, cv_result=True):\n",
    "\n",
    "    data.raw_ratings = train_set  \n",
    "\n",
    "    # grid search cross validation\n",
    "    gs = GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=n_cv, n_jobs=-1)\n",
    "    gs.fit(data)\n",
    "    best_algo = gs.best_estimator['rmse']\n",
    "    # hyper-parameters for the best RMSE score\n",
    "    best_params = gs.best_params['rmse']\n",
    "    print(\"Best hyper-parameters:\", best_params)\n",
    "    print()\n",
    "    \n",
    "    # show RMSE for each hyper-parameter combination if cv_result=True\n",
    "    # if cv_result=False, only show the RMSE for the best combination\n",
    "    if cv_result:\n",
    "        print(\"Training set RMSE for each hyper-parameter combination:\")\n",
    "    else:\n",
    "        print(\"Training set RMSE for the best hyper-parameter combination:\")\n",
    "    means = gs.cv_results['mean_test_rmse']\n",
    "    stds = gs.cv_results['std_test_rmse']\n",
    "    parameters = gs.cv_results['params']\n",
    "    for mean, std, params in zip(means, stds, parameters):\n",
    "        if (cv_result)|(params == best_params):\n",
    "            print(\"%0.4f (+/-%0.04f) for %r\" % (mean, std * 2, params))                \n",
    "    print()\n",
    "\n",
    "    # Compute performance on the whole training set \n",
    "    trainset = data.build_full_trainset()\n",
    "    best_algo.fit(trainset)\n",
    "    pred = best_algo.test(trainset.build_testset())\n",
    "#    print('Train set', end='  ')\n",
    "#    accuracy.rmse(pred)\n",
    "    \n",
    "    # Compute performance on test set\n",
    "    testset = data.construct_testset(test_set)  # testset is now the set B\n",
    "    pred = best_algo.test(testset)\n",
    "    print('Test set', end='  ')\n",
    "    accuracy.rmse(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise GridSearchCV accepts only classes, so I added all hyper-parameters (e.g., random_state) of an algorithm as a grid search parameter whether it is tuned or not. https://github.com/NicolasHug/Surprise/issues/212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### Basic Algorithms\n",
    "\n",
    "I will first try the most basic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NormalPredictor algorithm\n",
    "\n",
    "NormalPredictor predicts ratings randomly from the normal distribution with mean and standard deviation estimated by the training set. This is a good base model to be compared with more complex models. Click [here](https://surprise.readthedocs.io/en/stable/basic_algorithms.html) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.6269 (+/-0.0041) for {}\n",
      "\n",
      "Test set  RMSE: 1.6317\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, NormalPredictor, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just mean algorithm\n",
    "\n",
    "This algorithm uses just the mean of all ratings in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7656181070107047"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of the training set\n",
    "ratings = [r for _, _, r, _ in train_raw_ratings]\n",
    "sum(ratings)/len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Just_mean(AlgoBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return 3.7656\n",
    "\n",
    "#algo = Just_mean()\n",
    "#cross_validate(algo, data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.2353 (+/-0.0044) for {}\n",
      "\n",
      "Test set  RMSE: 1.2362\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, Just_mean, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simpler model using only mean is much better than the normal predictor with randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### Collaborative Filtering\n",
    "\n",
    "The collaborative filtering algorithms predict preference or ratings of a user on an item using preference or ratings of other users. These do not utilize metadata of items or users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Algorithm\n",
    "\n",
    "The [baseline algorithm](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration) predicts ratings using the mean ratings plus user and item biases, parameters to be optimized. It also has the regularization term with squares of the biases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1062 (+/-0.0029) for {}\n",
      "\n",
      "Estimating biases using als...\n",
      "Test set  RMSE: 1.0941\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, BaselineOnly, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The baseline model is much better than the Just_mean model! With tuning, this algorithm is expected to be improved even further.\n",
    "- By default, the above used Alternating Least Squares (ALS) and I am going to check how Stochastic Gradient Descent (SGD) peforms.\n",
    "- I also tried n_cv=5 for many algorithms, but it did not improve the performance on the test set although the performanceo on the training set is a little better with 5. Thus, I decided just use 3 since it saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'bsl_options': {'method': 'sgd'}}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1063 (+/-0.0027) for {'bsl_options': {'method': 'als'}}\n",
      "1.1005 (+/-0.0025) for {'bsl_options': {'method': 'sgd'}}\n",
      "\n",
      "Estimating biases using sgd...\n",
      "Test set  RMSE: 1.0891\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'bsl_options':{'method': ['als','sgd']}}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, BaselineOnly, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is slightly better (lower RMSE) than the default method ALS for the baseline model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'bsl_options': {'method': 'sgd', 'n_epochs': 20}}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1078 (+/-0.0031) for {'bsl_options': {'method': 'sgd', 'n_epochs': 10}}\n",
      "1.0999 (+/-0.0025) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20}}\n",
      "1.1002 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30}}\n",
      "\n",
      "Estimating biases using sgd...\n",
      "Test set  RMSE: 1.0891\n",
      "Wall time: 59.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'bsl_options':{'method': ['sgd'],'n_epochs': [10,20,30]}}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, BaselineOnly, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1130 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.05, 'learning_rate': 0.002}}\n",
      "1.1002 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1024 (+/-0.0026) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.05, 'learning_rate': 0.01}}\n",
      "1.1137 (+/-0.0022) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.1, 'learning_rate': 0.002}}\n",
      "1.1004 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1017 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.1, 'learning_rate': 0.01}}\n",
      "1.1146 (+/-0.0021) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.15, 'learning_rate': 0.002}}\n",
      "1.1009 (+/-0.0022) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1015 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.15, 'learning_rate': 0.01}}\n",
      "1.1051 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.05, 'learning_rate': 0.002}}\n",
      "1.1002 (+/-0.0025) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1059 (+/-0.0026) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.05, 'learning_rate': 0.01}}\n",
      "1.1057 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.1, 'learning_rate': 0.002}}\n",
      "1.0999 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1045 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.1, 'learning_rate': 0.01}}\n",
      "1.1065 (+/-0.0022) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.15, 'learning_rate': 0.002}}\n",
      "1.1001 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1038 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.15, 'learning_rate': 0.01}}\n",
      "1.1015 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.05, 'learning_rate': 0.002}}\n",
      "1.1018 (+/-0.0026) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1083 (+/-0.0027) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.05, 'learning_rate': 0.01}}\n",
      "1.1019 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.1, 'learning_rate': 0.002}}\n",
      "1.1012 (+/-0.0024) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1065 (+/-0.0025) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.1, 'learning_rate': 0.01}}\n",
      "1.1027 (+/-0.0022) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.15, 'learning_rate': 0.002}}\n",
      "1.1010 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1054 (+/-0.0023) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.15, 'learning_rate': 0.01}}\n",
      "\n",
      "Estimating biases using sgd...\n",
      "Test set  RMSE: 1.0897\n",
      "Wall time: 7min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'bsl_options':{'method': ['sgd'],'n_epochs': [20, 30, 40],\n",
    "                            'reg':[.05,.1,.15], 'learning_rate':[.002,.005,.01]}}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, BaselineOnly, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.0999 (+/-0.0046) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.01, 'learning_rate': 0.005}}\n",
      "1.0999 (+/-0.0046) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.015, 'learning_rate': 0.005}}\n",
      "1.0999 (+/-0.0046) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.02, 'learning_rate': 0.005}}\n",
      "1.0998 (+/-0.0046) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1000 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1006 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1015 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 20, 'reg': 0.2, 'learning_rate': 0.005}}\n",
      "1.1005 (+/-0.0046) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.01, 'learning_rate': 0.005}}\n",
      "1.1004 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.015, 'learning_rate': 0.005}}\n",
      "1.1003 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.02, 'learning_rate': 0.005}}\n",
      "1.0999 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.0996 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.0998 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1004 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 30, 'reg': 0.2, 'learning_rate': 0.005}}\n",
      "1.1026 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.01, 'learning_rate': 0.005}}\n",
      "1.1025 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.015, 'learning_rate': 0.005}}\n",
      "1.1023 (+/-0.0045) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.02, 'learning_rate': 0.005}}\n",
      "1.1016 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1009 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1008 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1010 (+/-0.0042) for {'bsl_options': {'method': 'sgd', 'n_epochs': 40, 'reg': 0.2, 'learning_rate': 0.005}}\n",
      "1.1048 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.01, 'learning_rate': 0.005}}\n",
      "1.1046 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.015, 'learning_rate': 0.005}}\n",
      "1.1044 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.02, 'learning_rate': 0.005}}\n",
      "1.1035 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1024 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1020 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1020 (+/-0.0042) for {'bsl_options': {'method': 'sgd', 'n_epochs': 50, 'reg': 0.2, 'learning_rate': 0.005}}\n",
      "1.1084 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.01, 'learning_rate': 0.005}}\n",
      "1.1081 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.015, 'learning_rate': 0.005}}\n",
      "1.1078 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.02, 'learning_rate': 0.005}}\n",
      "1.1065 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1049 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1040 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1036 (+/-0.0042) for {'bsl_options': {'method': 'sgd', 'n_epochs': 70, 'reg': 0.2, 'learning_rate': 0.005}}\n",
      "1.1115 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.01, 'learning_rate': 0.005}}\n",
      "1.1112 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.015, 'learning_rate': 0.005}}\n",
      "1.1108 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.02, 'learning_rate': 0.005}}\n",
      "1.1091 (+/-0.0044) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.05, 'learning_rate': 0.005}}\n",
      "1.1071 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.1, 'learning_rate': 0.005}}\n",
      "1.1058 (+/-0.0043) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.15, 'learning_rate': 0.005}}\n",
      "1.1050 (+/-0.0042) for {'bsl_options': {'method': 'sgd', 'n_epochs': 100, 'reg': 0.2, 'learning_rate': 0.005}}\n",
      "\n",
      "Estimating biases using sgd...\n",
      "Test set  RMSE: 1.0897\n",
      "Wall time: 11min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'bsl_options':{'method': ['sgd'],'n_epochs': [20,30,40,50,70,100],\n",
    "                            'reg':[.01,.015,.02,.05,.1,.15,.2], 'learning_rate':[.005]}}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, BaselineOnly, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD algorithm\n",
    "\n",
    "The Singular value decomposition (SVD), a matrix factorization method, is a popular collaborative filtering algorithm done on the user-item rating matrix. The formulas for SVD in the Surprise package are [here](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) and also click [here](https://medium.com/@m_n_malaeb/singular-value-decomposition-svd-in-recommender-systems-for-non-math-statistics-programming-4a622de653e9) for easy explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1151 (+/-0.0057) for {'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.1066\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD without tuning hyper-parameters is not better than the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1367 (+/-0.0066) for {'n_epochs': 10, 'lr_all': 0.002, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1364 (+/-0.0066) for {'n_epochs': 10, 'lr_all': 0.002, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1364 (+/-0.0066) for {'n_epochs': 10, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1149 (+/-0.0063) for {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1133 (+/-0.0063) for {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1124 (+/-0.0063) for {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1147 (+/-0.0059) for {'n_epochs': 10, 'lr_all': 0.01, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1080 (+/-0.0059) for {'n_epochs': 10, 'lr_all': 0.01, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1045 (+/-0.0059) for {'n_epochs': 10, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1188 (+/-0.0065) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1177 (+/-0.0064) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1171 (+/-0.0064) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1148 (+/-0.0060) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1078 (+/-0.0059) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1042 (+/-0.0059) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1434 (+/-0.0055) for {'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1213 (+/-0.0055) for {'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1072 (+/-0.0057) for {'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1129 (+/-0.0063) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1105 (+/-0.0063) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1092 (+/-0.0062) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1288 (+/-0.0057) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1130 (+/-0.0057) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1044 (+/-0.0058) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1586 (+/-0.0052) for {'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.02, 'random_state': 32}\n",
      "1.1332 (+/-0.0053) for {'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.05, 'random_state': 32}\n",
      "1.1136 (+/-0.0056) for {'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0920\n",
      "Wall time: 9min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[10, 20, 30], 'lr_all': [0.002, 0.005,.01], \n",
    "              'reg_all': [0.02, 0.05, .1], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1247 (+/-0.0007) for {'n_epochs': 30, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1254 (+/-0.0008) for {'n_epochs': 30, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1271 (+/-0.0009) for {'n_epochs': 30, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1313 (+/-0.0010) for {'n_epochs': 30, 'lr_all': 0.001, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1094 (+/-0.0012) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1098 (+/-0.0013) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1116 (+/-0.0013) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1168 (+/-0.0013) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1045 (+/-0.0020) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1022 (+/-0.0019) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1035 (+/-0.0018) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1084 (+/-0.0017) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1126 (+/-0.0011) for {'n_epochs': 50, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1132 (+/-0.0011) for {'n_epochs': 50, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1150 (+/-0.0012) for {'n_epochs': 50, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1200 (+/-0.0013) for {'n_epochs': 50, 'lr_all': 0.001, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1042 (+/-0.0017) for {'n_epochs': 50, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1036 (+/-0.0016) for {'n_epochs': 50, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1054 (+/-0.0016) for {'n_epochs': 50, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1106 (+/-0.0016) for {'n_epochs': 50, 'lr_all': 0.002, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1102 (+/-0.0024) for {'n_epochs': 50, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1031 (+/-0.0021) for {'n_epochs': 50, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1037 (+/-0.0020) for {'n_epochs': 50, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1079 (+/-0.0018) for {'n_epochs': 50, 'lr_all': 0.005, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1072 (+/-0.0013) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1074 (+/-0.0014) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1093 (+/-0.0014) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1145 (+/-0.0014) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1041 (+/-0.0020) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1022 (+/-0.0018) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1036 (+/-0.0018) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1086 (+/-0.0017) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1159 (+/-0.0026) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1046 (+/-0.0021) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1045 (+/-0.0020) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1082 (+/-0.0019) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1048 (+/-0.0016) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1045 (+/-0.0016) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1063 (+/-0.0016) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1116 (+/-0.0015) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1057 (+/-0.0022) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1021 (+/-0.0020) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1032 (+/-0.0019) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1080 (+/-0.0018) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.5, 'random_state': 32}\n",
      "1.1192 (+/-0.0026) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1059 (+/-0.0021) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1051 (+/-0.0020) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1084 (+/-0.0019) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.5, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0917\n",
      "Wall time: 34min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[30,50,70,90], 'lr_all': [.001,.002, 0.005], \n",
    "              'reg_all': [ .1, .2, .3, .5], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1054 (+/-0.0022) for {'n_epochs': 80, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1055 (+/-0.0021) for {'n_epochs': 80, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1074 (+/-0.0021) for {'n_epochs': 80, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1042 (+/-0.0025) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1017 (+/-0.0024) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1031 (+/-0.0023) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1168 (+/-0.0028) for {'n_epochs': 80, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1049 (+/-0.0028) for {'n_epochs': 80, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1045 (+/-0.0025) for {'n_epochs': 80, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1044 (+/-0.0022) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1043 (+/-0.0021) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1061 (+/-0.0021) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1051 (+/-0.0025) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1018 (+/-0.0024) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1030 (+/-0.0023) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1182 (+/-0.0028) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1055 (+/-0.0028) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1048 (+/-0.0025) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1038 (+/-0.0023) for {'n_epochs': 100, 'lr_all': 0.001, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1034 (+/-0.0022) for {'n_epochs': 100, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1052 (+/-0.0021) for {'n_epochs': 100, 'lr_all': 0.001, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1063 (+/-0.0026) for {'n_epochs': 100, 'lr_all': 0.002, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1020 (+/-0.0025) for {'n_epochs': 100, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1030 (+/-0.0024) for {'n_epochs': 100, 'lr_all': 0.002, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1193 (+/-0.0028) for {'n_epochs': 100, 'lr_all': 0.005, 'reg_all': 0.1, 'random_state': 32}\n",
      "1.1061 (+/-0.0029) for {'n_epochs': 100, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1050 (+/-0.0025) for {'n_epochs': 100, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0916\n",
      "Wall time: 29min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[80,90,100], 'lr_all': [.001,.002, 0.005], \n",
    "              'reg_all': [ .1, .2, .3], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1068 (+/-0.0063) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1073 (+/-0.0063) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1081 (+/-0.0063) for {'n_epochs': 70, 'lr_all': 0.001, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1022 (+/-0.0058) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1022 (+/-0.0059) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1027 (+/-0.0060) for {'n_epochs': 70, 'lr_all': 0.002, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1075 (+/-0.0055) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1048 (+/-0.0058) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1043 (+/-0.0059) for {'n_epochs': 70, 'lr_all': 0.005, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1052 (+/-0.0062) for {'n_epochs': 80, 'lr_all': 0.001, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1056 (+/-0.0062) for {'n_epochs': 80, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1064 (+/-0.0062) for {'n_epochs': 80, 'lr_all': 0.001, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1023 (+/-0.0057) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1021 (+/-0.0059) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1026 (+/-0.0059) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1088 (+/-0.0056) for {'n_epochs': 80, 'lr_all': 0.005, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1055 (+/-0.0058) for {'n_epochs': 80, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1048 (+/-0.0059) for {'n_epochs': 80, 'lr_all': 0.005, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1040 (+/-0.0061) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1044 (+/-0.0061) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1052 (+/-0.0061) for {'n_epochs': 90, 'lr_all': 0.001, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1026 (+/-0.0057) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1022 (+/-0.0058) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1026 (+/-0.0059) for {'n_epochs': 90, 'lr_all': 0.002, 'reg_all': 0.25, 'random_state': 32}\n",
      "1.1100 (+/-0.0056) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1062 (+/-0.0058) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1052 (+/-0.0059) for {'n_epochs': 90, 'lr_all': 0.005, 'reg_all': 0.25, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0916\n",
      "Wall time: 26min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[70,80,90], 'lr_all': [.001,.002, 0.005], \n",
    "              'reg_all': [ .15, .2, .25], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.18, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1016 (+/-0.0048) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1016 (+/-0.0048) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1017 (+/-0.0048) for {'n_epochs': 80, 'lr_all': 0.002, 'reg_all': 0.22, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0914\n",
      "Wall time: 4min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[80], 'lr_all': [.002], \n",
    "              'reg_all': [ .18, .2, .22], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found some smaller number of epochs are as good as big ones below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1024 (+/-0.0037) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.15, 'random_state': 32}\n",
      "1.1022 (+/-0.0036) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1035 (+/-0.0034) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.3, 'random_state': 32}\n",
      "1.1084 (+/-0.0032) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.5, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0916\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[30], 'lr_all': [0.005], \n",
    "              'reg_all': [.15,.2,.3,.5], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.18, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1021 (+/-0.0028) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1021 (+/-0.0027) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1026 (+/-0.0027) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.25, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0914\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[30], 'lr_all': [0.005], \n",
    "              'reg_all': [.18,.2,.25], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.18, 'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1176 (+/-0.0034) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.17, 'random_state': 32}\n",
      "1.1177 (+/-0.0034) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1178 (+/-0.0034) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.19, 'random_state': 32}\n",
      "1.1180 (+/-0.0034) for {'n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1035 (+/-0.0029) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.17, 'random_state': 32}\n",
      "1.1035 (+/-0.0028) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1036 (+/-0.0028) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.19, 'random_state': 32}\n",
      "1.1037 (+/-0.0028) for {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1094 (+/-0.0032) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.17, 'random_state': 32}\n",
      "1.1095 (+/-0.0031) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1096 (+/-0.0031) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.19, 'random_state': 32}\n",
      "1.1098 (+/-0.0031) for {'n_epochs': 30, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1021 (+/-0.0027) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.17, 'random_state': 32}\n",
      "1.1021 (+/-0.0027) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1021 (+/-0.0027) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.19, 'random_state': 32}\n",
      "1.1021 (+/-0.0027) for {'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1054 (+/-0.0030) for {'n_epochs': 40, 'lr_all': 0.002, 'reg_all': 0.17, 'random_state': 32}\n",
      "1.1055 (+/-0.0030) for {'n_epochs': 40, 'lr_all': 0.002, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1056 (+/-0.0030) for {'n_epochs': 40, 'lr_all': 0.002, 'reg_all': 0.19, 'random_state': 32}\n",
      "1.1057 (+/-0.0030) for {'n_epochs': 40, 'lr_all': 0.002, 'reg_all': 0.2, 'random_state': 32}\n",
      "1.1026 (+/-0.0027) for {'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.17, 'random_state': 32}\n",
      "1.1025 (+/-0.0027) for {'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.18, 'random_state': 32}\n",
      "1.1024 (+/-0.0027) for {'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.19, 'random_state': 32}\n",
      "1.1024 (+/-0.0027) for {'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.2, 'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.0914\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_epochs':[20,30,40], 'lr_all': [.002, 0.005], \n",
    "              'reg_all': [.17,.18,.19,.2], 'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVD, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVDpp algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1175 (+/-0.0042) for {'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.1108\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, SVDpp, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVDpp is so much slower (9 times!!) and higher in RMSE than SVD when both are not tuned! Thus, I will not further tune this algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.2139 (+/-0.0019) for {'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.1755\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'random_state':[32]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, NMF, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NMF is much worse in RMSE than SVD when not tuned, so I will not further tune this algorithm.\n",
    "- I also tried 4 KNN-based algorithms in the Surprise, but they threw a memory error while I'm using only 50% of 24GB memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'random_state': 32}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1643 (+/-0.0033) for {'random_state': 32}\n",
      "\n",
      "Test set  RMSE: 1.1332\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'random_state':[32]} #n_epochs=20 by default\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, CoClustering, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'random_state': 32, 'n_epochs': 30}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1634 (+/-0.0042) for {'random_state': 32, 'n_epochs': 30}\n",
      "1.1636 (+/-0.0044) for {'random_state': 32, 'n_epochs': 40}\n",
      "1.1638 (+/-0.0046) for {'random_state': 32, 'n_epochs': 50}\n",
      "\n",
      "Test set  RMSE: 1.1333\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'random_state':[32], 'n_epochs':[30,40,50]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, CoClustering, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'random_state': 32, 'n_cltr_u': 3, 'n_cltr_i': 3}\n",
      "\n",
      "Training set RMSE for each hyper-parameter combination:\n",
      "1.1636 (+/-0.0053) for {'random_state': 32, 'n_cltr_u': 3, 'n_cltr_i': 3}\n",
      "1.1662 (+/-0.0060) for {'random_state': 32, 'n_cltr_u': 3, 'n_cltr_i': 4}\n",
      "1.1688 (+/-0.0048) for {'random_state': 32, 'n_cltr_u': 3, 'n_cltr_i': 5}\n",
      "1.1712 (+/-0.0030) for {'random_state': 32, 'n_cltr_u': 4, 'n_cltr_i': 3}\n",
      "1.1692 (+/-0.0044) for {'random_state': 32, 'n_cltr_u': 4, 'n_cltr_i': 4}\n",
      "1.1746 (+/-0.0028) for {'random_state': 32, 'n_cltr_u': 4, 'n_cltr_i': 5}\n",
      "1.1780 (+/-0.0037) for {'random_state': 32, 'n_cltr_u': 5, 'n_cltr_i': 3}\n",
      "1.1848 (+/-0.0057) for {'random_state': 32, 'n_cltr_u': 5, 'n_cltr_i': 4}\n",
      "1.1818 (+/-0.0056) for {'random_state': 32, 'n_cltr_u': 5, 'n_cltr_i': 5}\n",
      "\n",
      "Test set  RMSE: 1.1332\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'random_state':[32], 'n_cltr_u':[3,4,5], 'n_cltr_i':[3,4,5]}\n",
    "training_test_algorithm(train_raw_ratings, test_raw_ratings, CoClustering, param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### Content-Based Filtering \n",
    "\n",
    "The Content-based filtering algorithms use item or user's metadata (e.g., content of films or demographic profile of users) to predict ratings or preferences for recommendations. Here I will use the data for businesses to predict star ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 493658 entries, 62 to 4017804\n",
      "Data columns (total 17 columns):\n",
      "review_id                493658 non-null object\n",
      "user_id                  493658 non-null object\n",
      "business_id              493658 non-null object\n",
      "stars                    493658 non-null int64\n",
      "date                     493658 non-null object\n",
      "business_name            493658 non-null object\n",
      "city                     493658 non-null object\n",
      "state                    493658 non-null object\n",
      "city_state               493658 non-null object\n",
      "latitude                 493658 non-null float64\n",
      "longitude                493658 non-null float64\n",
      "price_range              493658 non-null float64\n",
      "business_review_count    493658 non-null int64\n",
      "categories               493658 non-null object\n",
      "user_name                493658 non-null object\n",
      "yelping_since            493658 non-null object\n",
      "user_review_count        493658 non-null int64\n",
      "dtypes: float64(3), int64(3), object(11)\n",
      "memory usage: 87.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_Vegas_over_10_20.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the following business features for the recommender system. \n",
    "- city (now all in Nevada, so state name is unnecessary)\n",
    "- lattitude\n",
    "- longitude\n",
    "- prince_range\n",
    "- business_review_count\n",
    "- categories\n",
    "\n",
    "The city name and categories need to be preprocessed since they are strings and list of strings, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove uncessary columns\n",
    "#df = df.drop([], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# strings back to lists (i.e., remove '' outside [])\n",
    "df_Vegas_over_10_20.loc[:,'categories'] = df_Vegas_over_10_20.categories.apply(lambda x: literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nightlife', 'Japanese', 'Karaoke', 'Sushi Bars', 'Restaurants'] <class 'list'>\n",
      "['Beer Gardens', 'Nightlife', 'Thai', 'Restaurants'] <class 'list'>\n",
      "['Nightlife', 'Pizza', 'Wine Bars', 'Bars', 'Italian', 'Restaurants'] <class 'list'>\n",
      "['Noodles', 'Japanese', 'Restaurants', 'Ramen'] <class 'list'>\n",
      "['Live/Raw Food', 'Bars', 'Nightlife', 'Cocktail Bars', 'Sushi Bars', 'Seafood', 'Restaurants'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check if the transformation was well done\n",
    "for row in df_Vegas_over_10_20.categories[:5]:\n",
    "    print(row, type(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = defaultdict(list)\n",
    "for category in category_list:\n",
    "    condition = df_Vegas_over_10_20.categories.apply(lambda x: (category in x))\n",
    "    business_count = df_Vegas_over_10_20[condition].business_id.nunique()\n",
    "    review_count = sum(condition)\n",
    "    category_counts[category] = [business_count, review_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 439 categories over all. I will reduce this number since each of categories will become a feature of businesses. To do this, I am going to check how many businesses each category was used for and remove infrequent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts_df = pd.DataFrame.from_dict(category_counts, orient='index')\n",
    "category_counts_df.columns =['business_count','review_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_count</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Restaurants</th>\n",
       "      <td>5211</td>\n",
       "      <td>445901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>2218</td>\n",
       "      <td>153104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nightlife</th>\n",
       "      <td>869</td>\n",
       "      <td>92369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American (Traditional)</th>\n",
       "      <td>830</td>\n",
       "      <td>79669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bars</th>\n",
       "      <td>820</td>\n",
       "      <td>88944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fast Food</th>\n",
       "      <td>748</td>\n",
       "      <td>28088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexican</th>\n",
       "      <td>672</td>\n",
       "      <td>41049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American (New)</th>\n",
       "      <td>602</td>\n",
       "      <td>77934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sandwiches</th>\n",
       "      <td>584</td>\n",
       "      <td>44482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pizza</th>\n",
       "      <td>564</td>\n",
       "      <td>37414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coffee &amp; Tea</th>\n",
       "      <td>539</td>\n",
       "      <td>33990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breakfast &amp; Brunch</th>\n",
       "      <td>537</td>\n",
       "      <td>67998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burgers</th>\n",
       "      <td>528</td>\n",
       "      <td>41843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>391</td>\n",
       "      <td>27282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>390</td>\n",
       "      <td>35774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desserts</th>\n",
       "      <td>373</td>\n",
       "      <td>36576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seafood</th>\n",
       "      <td>363</td>\n",
       "      <td>44022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>361</td>\n",
       "      <td>42556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specialty Food</th>\n",
       "      <td>308</td>\n",
       "      <td>20150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sushi Bars</th>\n",
       "      <td>275</td>\n",
       "      <td>31939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian Fusion</th>\n",
       "      <td>275</td>\n",
       "      <td>31374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bakeries</th>\n",
       "      <td>269</td>\n",
       "      <td>21829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salad</th>\n",
       "      <td>262</td>\n",
       "      <td>24836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shopping</th>\n",
       "      <td>252</td>\n",
       "      <td>10310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ice Cream &amp; Frozen Yogurt</th>\n",
       "      <td>250</td>\n",
       "      <td>14957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Juice Bars &amp; Smoothies</th>\n",
       "      <td>243</td>\n",
       "      <td>17347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steakhouses</th>\n",
       "      <td>242</td>\n",
       "      <td>31804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicken Wings</th>\n",
       "      <td>235</td>\n",
       "      <td>11764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cafes</th>\n",
       "      <td>231</td>\n",
       "      <td>23732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Bars</th>\n",
       "      <td>222</td>\n",
       "      <td>15582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greek</th>\n",
       "      <td>56</td>\n",
       "      <td>6282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southern</th>\n",
       "      <td>56</td>\n",
       "      <td>6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagels</th>\n",
       "      <td>55</td>\n",
       "      <td>4305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Eastern</th>\n",
       "      <td>55</td>\n",
       "      <td>4894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tapas/Small Plates</th>\n",
       "      <td>54</td>\n",
       "      <td>7942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beauty &amp; Spas</th>\n",
       "      <td>52</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department Stores</th>\n",
       "      <td>51</td>\n",
       "      <td>1917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health Markets</th>\n",
       "      <td>50</td>\n",
       "      <td>2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>49</td>\n",
       "      <td>3732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tapas Bars</th>\n",
       "      <td>48</td>\n",
       "      <td>6675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shaved Ice</th>\n",
       "      <td>46</td>\n",
       "      <td>3217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comfort Food</th>\n",
       "      <td>45</td>\n",
       "      <td>5949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gastropubs</th>\n",
       "      <td>44</td>\n",
       "      <td>6793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breweries</th>\n",
       "      <td>43</td>\n",
       "      <td>4659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ramen</th>\n",
       "      <td>43</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Automotive</th>\n",
       "      <td>43</td>\n",
       "      <td>2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tacos</th>\n",
       "      <td>43</td>\n",
       "      <td>3221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chocolatiers &amp; Shops</th>\n",
       "      <td>42</td>\n",
       "      <td>3012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venues &amp; Event Spaces</th>\n",
       "      <td>42</td>\n",
       "      <td>5523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cupcakes</th>\n",
       "      <td>42</td>\n",
       "      <td>3026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hotels</th>\n",
       "      <td>41</td>\n",
       "      <td>3953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosmetics &amp; Beauty Supply</th>\n",
       "      <td>41</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Candy Stores</th>\n",
       "      <td>40</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poke</th>\n",
       "      <td>38</td>\n",
       "      <td>3289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cajun/Creole</th>\n",
       "      <td>38</td>\n",
       "      <td>5189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beer Bar</th>\n",
       "      <td>37</td>\n",
       "      <td>4517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flowers &amp; Gifts</th>\n",
       "      <td>36</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karaoke</th>\n",
       "      <td>36</td>\n",
       "      <td>3087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom Cakes</th>\n",
       "      <td>36</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music Venues</th>\n",
       "      <td>35</td>\n",
       "      <td>3709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           business_count  review_count\n",
       "Restaurants                          5211        445901\n",
       "Food                                 2218        153104\n",
       "Nightlife                             869         92369\n",
       "American (Traditional)                830         79669\n",
       "Bars                                  820         88944\n",
       "Fast Food                             748         28088\n",
       "Mexican                               672         41049\n",
       "American (New)                        602         77934\n",
       "Sandwiches                            584         44482\n",
       "Pizza                                 564         37414\n",
       "Coffee & Tea                          539         33990\n",
       "Breakfast & Brunch                    537         67998\n",
       "Burgers                               528         41843\n",
       "Chinese                               391         27282\n",
       "Italian                               390         35774\n",
       "Desserts                              373         36576\n",
       "Seafood                               363         44022\n",
       "Japanese                              361         42556\n",
       "Specialty Food                        308         20150\n",
       "Sushi Bars                            275         31939\n",
       "Asian Fusion                          275         31374\n",
       "Bakeries                              269         21829\n",
       "Salad                                 262         24836\n",
       "Shopping                              252         10310\n",
       "Ice Cream & Frozen Yogurt             250         14957\n",
       "Juice Bars & Smoothies                243         17347\n",
       "Steakhouses                           242         31804\n",
       "Chicken Wings                         235         11764\n",
       "Cafes                                 231         23732\n",
       "Sports Bars                           222         15582\n",
       "...                                   ...           ...\n",
       "Greek                                  56          6282\n",
       "Southern                               56          6620\n",
       "Bagels                                 55          4305\n",
       "Middle Eastern                         55          4894\n",
       "Tapas/Small Plates                     54          7942\n",
       "Beauty & Spas                          52          1499\n",
       "Department Stores                      51          1917\n",
       "Health Markets                         50          2383\n",
       "Indian                                 49          3732\n",
       "Tapas Bars                             48          6675\n",
       "Shaved Ice                             46          3217\n",
       "Comfort Food                           45          5949\n",
       "Gastropubs                             44          6793\n",
       "Breweries                              43          4659\n",
       "Ramen                                  43          6692\n",
       "Automotive                             43          2251\n",
       "Tacos                                  43          3221\n",
       "Chocolatiers & Shops                   42          3012\n",
       "Venues & Event Spaces                  42          5523\n",
       "Cupcakes                               42          3026\n",
       "Hotels                                 41          3953\n",
       "Cosmetics & Beauty Supply              41           890\n",
       "Candy Stores                           40          1989\n",
       "Poke                                   38          3289\n",
       "Cajun/Creole                           38          5189\n",
       "Beer Bar                               37          4517\n",
       "Flowers & Gifts                        36          2050\n",
       "Karaoke                                36          3087\n",
       "Custom Cakes                           36          1849\n",
       "Music Venues                           35          3709\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top categories sorted by number of businesses\n",
    "category_counts_df.sort_values(by='business_count', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top categories sorted by number of reviews\n",
    "#category_counts_df.sort_values(by='review_count', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of categories ovelapping in the top business and top review categories\n",
    "top_business_categories = category_counts_df.sort_values(by='business_count', ascending=False)[:100].index\n",
    "top_review_categories = category_counts_df.sort_values(by='review_count', ascending=False)[:100].index\n",
    "len(set(top_business_categories).intersection(set(top_review_categories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use the top 100 categories in businesses. 91 of them are also in the top review categories. The df_Vegas_over_10_20 now will be extended with 100 more columns, one for each category. Each category column will contain 1 if the review is given for a business with the corresponding category and 0, otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in top_business_categories:\n",
    "    df_Vegas_over_10_20[category] = df_Vegas_over_10_20.categories.apply(lambda x: category in x)*1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>city_state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Hotels</th>\n",
       "      <th>Cosmetics &amp; Beauty Supply</th>\n",
       "      <th>Candy Stores</th>\n",
       "      <th>Poke</th>\n",
       "      <th>Cajun/Creole</th>\n",
       "      <th>Beer Bar</th>\n",
       "      <th>Flowers &amp; Gifts</th>\n",
       "      <th>Karaoke</th>\n",
       "      <th>Custom Cakes</th>\n",
       "      <th>Music Venues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>66FSK2J1uHJqVI1Z8QiFGw</td>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>6tSvz_21BMo3a4GaItwa0g</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>Jjanga Japanese Restaurant</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.123557</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>smfJ-FC5iokX7AbGbTMD4Q</td>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>-ed0Yc9on37RoIoG2ZgxBA</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>Le Thai</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.168802</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1QR6BgzXhpo7ngGEw-BW4w</td>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>XiN6fI8I3Mzg2nPRJ9ukRQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-09-17</td>\n",
       "      <td>Prosecco Fresh Italian Kitchen</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.028232</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 review_id                 user_id             business_id  \\\n",
       "62  66FSK2J1uHJqVI1Z8QiFGw  Yy_iGXxLpL6tYDQoE-6XVg  6tSvz_21BMo3a4GaItwa0g   \n",
       "63  smfJ-FC5iokX7AbGbTMD4Q  Yy_iGXxLpL6tYDQoE-6XVg  -ed0Yc9on37RoIoG2ZgxBA   \n",
       "64  1QR6BgzXhpo7ngGEw-BW4w  Yy_iGXxLpL6tYDQoE-6XVg  XiN6fI8I3Mzg2nPRJ9ukRQ   \n",
       "\n",
       "    stars        date                   business_name       city state  \\\n",
       "62      4  2017-04-08      Jjanga Japanese Restaurant  Las Vegas    NV   \n",
       "63      3  2017-03-22                         Le Thai  Las Vegas    NV   \n",
       "64      5  2017-09-17  Prosecco Fresh Italian Kitchen  Las Vegas    NV   \n",
       "\n",
       "       city_state   latitude      ...       Hotels  Cosmetics & Beauty Supply  \\\n",
       "62  Las Vegas, NV  36.123557      ...            0                          0   \n",
       "63  Las Vegas, NV  36.168802      ...            0                          0   \n",
       "64  Las Vegas, NV  36.028232      ...            0                          0   \n",
       "\n",
       "    Candy Stores Poke Cajun/Creole Beer Bar  Flowers & Gifts  Karaoke  \\\n",
       "62             0    0            0        0                0        1   \n",
       "63             0    0            0        0                0        0   \n",
       "64             0    0            0        0                0        0   \n",
       "\n",
       "    Custom Cakes  Music Venues  \n",
       "62             0             0  \n",
       "63             0             0  \n",
       "64             0             0  \n",
       "\n",
       "[3 rows x 117 columns]"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas_over_10_20.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445901"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas_over_10_20.Restaurants.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### City "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df_Vegas_over_10_20.city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boulder City         1677\n",
       "Henderson           52024\n",
       "Las Vegas          429709\n",
       "North Las Vegas     10211\n",
       "Paradise               37\n",
       "dtype: int64"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Vegas_over_10_20 = df_Vegas_over_10_20.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>city_state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Beer Bar</th>\n",
       "      <th>Flowers &amp; Gifts</th>\n",
       "      <th>Karaoke</th>\n",
       "      <th>Custom Cakes</th>\n",
       "      <th>Music Venues</th>\n",
       "      <th>Boulder City</th>\n",
       "      <th>Henderson</th>\n",
       "      <th>Las Vegas</th>\n",
       "      <th>North Las Vegas</th>\n",
       "      <th>Paradise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>66FSK2J1uHJqVI1Z8QiFGw</td>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>6tSvz_21BMo3a4GaItwa0g</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>Jjanga Japanese Restaurant</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.123557</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>smfJ-FC5iokX7AbGbTMD4Q</td>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>-ed0Yc9on37RoIoG2ZgxBA</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>Le Thai</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.168802</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1QR6BgzXhpo7ngGEw-BW4w</td>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>XiN6fI8I3Mzg2nPRJ9ukRQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-09-17</td>\n",
       "      <td>Prosecco Fresh Italian Kitchen</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>36.028232</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 review_id                 user_id             business_id  \\\n",
       "62  66FSK2J1uHJqVI1Z8QiFGw  Yy_iGXxLpL6tYDQoE-6XVg  6tSvz_21BMo3a4GaItwa0g   \n",
       "63  smfJ-FC5iokX7AbGbTMD4Q  Yy_iGXxLpL6tYDQoE-6XVg  -ed0Yc9on37RoIoG2ZgxBA   \n",
       "64  1QR6BgzXhpo7ngGEw-BW4w  Yy_iGXxLpL6tYDQoE-6XVg  XiN6fI8I3Mzg2nPRJ9ukRQ   \n",
       "\n",
       "    stars        date                   business_name       city state  \\\n",
       "62      4  2017-04-08      Jjanga Japanese Restaurant  Las Vegas    NV   \n",
       "63      3  2017-03-22                         Le Thai  Las Vegas    NV   \n",
       "64      5  2017-09-17  Prosecco Fresh Italian Kitchen  Las Vegas    NV   \n",
       "\n",
       "       city_state   latitude    ...     Beer Bar  Flowers & Gifts  Karaoke  \\\n",
       "62  Las Vegas, NV  36.123557    ...            0                0        1   \n",
       "63  Las Vegas, NV  36.168802    ...            0                0        0   \n",
       "64  Las Vegas, NV  36.028232    ...            0                0        0   \n",
       "\n",
       "   Custom Cakes Music Venues Boulder City  Henderson  Las Vegas  \\\n",
       "62            0            0            0          0          1   \n",
       "63            0            0            0          0          1   \n",
       "64            0            0            0          0          1   \n",
       "\n",
       "    North Las Vegas  Paradise  \n",
       "62                0         0  \n",
       "63                0         0  \n",
       "64                0         0  \n",
       "\n",
       "[3 rows x 122 columns]"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Vegas_over_10_20.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_used = ['user_id', 'business_id', 'latitude', 'longitude', 'price_range', 'business_review_count'] \\\n",
    "               + list(top_business_categories) \\\n",
    "               + ['Boulder City', 'Henderson', 'Las Vegas', 'North Las Vegas', 'Paradise'] \\\n",
    "               + ['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample =df_Vegas_over_10_20[columns_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price_range</th>\n",
       "      <th>business_review_count</th>\n",
       "      <th>Restaurants</th>\n",
       "      <th>Food</th>\n",
       "      <th>Nightlife</th>\n",
       "      <th>American (Traditional)</th>\n",
       "      <th>...</th>\n",
       "      <th>Flowers &amp; Gifts</th>\n",
       "      <th>Karaoke</th>\n",
       "      <th>Custom Cakes</th>\n",
       "      <th>Music Venues</th>\n",
       "      <th>Boulder City</th>\n",
       "      <th>Henderson</th>\n",
       "      <th>Las Vegas</th>\n",
       "      <th>North Las Vegas</th>\n",
       "      <th>Paradise</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>6tSvz_21BMo3a4GaItwa0g</td>\n",
       "      <td>36.123557</td>\n",
       "      <td>-115.207538</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>-ed0Yc9on37RoIoG2ZgxBA</td>\n",
       "      <td>36.168802</td>\n",
       "      <td>-115.139880</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1590</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>XiN6fI8I3Mzg2nPRJ9ukRQ</td>\n",
       "      <td>36.028232</td>\n",
       "      <td>-115.114936</td>\n",
       "      <td>2.0</td>\n",
       "      <td>378</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>BEtgRzNeXGAf0uQ-HuSyfA</td>\n",
       "      <td>36.058065</td>\n",
       "      <td>-115.278394</td>\n",
       "      <td>2.0</td>\n",
       "      <td>361</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Yy_iGXxLpL6tYDQoE-6XVg</td>\n",
       "      <td>7wHLFohwCw8l6WS-feLjeg</td>\n",
       "      <td>36.122913</td>\n",
       "      <td>-115.279937</td>\n",
       "      <td>2.0</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_id             business_id   latitude   longitude  \\\n",
       "62  Yy_iGXxLpL6tYDQoE-6XVg  6tSvz_21BMo3a4GaItwa0g  36.123557 -115.207538   \n",
       "63  Yy_iGXxLpL6tYDQoE-6XVg  -ed0Yc9on37RoIoG2ZgxBA  36.168802 -115.139880   \n",
       "64  Yy_iGXxLpL6tYDQoE-6XVg  XiN6fI8I3Mzg2nPRJ9ukRQ  36.028232 -115.114936   \n",
       "65  Yy_iGXxLpL6tYDQoE-6XVg  BEtgRzNeXGAf0uQ-HuSyfA  36.058065 -115.278394   \n",
       "66  Yy_iGXxLpL6tYDQoE-6XVg  7wHLFohwCw8l6WS-feLjeg  36.122913 -115.279937   \n",
       "\n",
       "    price_range  business_review_count  Restaurants  Food  Nightlife  \\\n",
       "62          2.0                   1096            1     0          1   \n",
       "63          2.0                   1590            1     0          1   \n",
       "64          2.0                    378            1     0          1   \n",
       "65          2.0                    361            1     0          0   \n",
       "66          2.0                    555            1     0          1   \n",
       "\n",
       "    American (Traditional)  ...    Flowers & Gifts  Karaoke  Custom Cakes  \\\n",
       "62                       0  ...                  0        1             0   \n",
       "63                       0  ...                  0        0             0   \n",
       "64                       0  ...                  0        0             0   \n",
       "65                       0  ...                  0        0             0   \n",
       "66                       0  ...                  0        0             0   \n",
       "\n",
       "    Music Venues  Boulder City  Henderson  Las Vegas  North Las Vegas  \\\n",
       "62             0             0          0          1                0   \n",
       "63             0             0          0          1                0   \n",
       "64             0             0          0          1                0   \n",
       "65             0             0          0          1                0   \n",
       "66             0             0          0          1                0   \n",
       "\n",
       "    Paradise  stars  \n",
       "62         0      4  \n",
       "63         0      3  \n",
       "64         0      5  \n",
       "65         0      5  \n",
       "66         0      5  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493658"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_idx = list(range(len(df_sample)))\n",
    "random.Random(32).shuffle(shuffled_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[419483, 192516, 166277, 161478, 171773]"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% training and 10% test data\n",
    "threshold = int(.9 * len(df_sample))\n",
    "df_sample_train = (df_sample.iloc[shuffled_idx,:]).iloc[:threshold,:]\n",
    "df_sample_test = (df_sample.iloc[shuffled_idx,:]).iloc[threshold:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check if the split made the training and test sets same as before. This checking step is necessary for fair comparison between collaborative and content-based filterings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3528833</th>\n",
       "      <td>U4INQZOPSUaj8hMjLlZ3KA</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966887</th>\n",
       "      <td>cg2P244yON3-_GXWkgAgsw</td>\n",
       "      <td>umXvdus9LbC6oxtLdXelFQ</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777578</th>\n",
       "      <td>YHdXkAmndIfuIczWOnsjeQ</td>\n",
       "      <td>7HIa2lYy5jgcZuADlRjKSg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740381</th>\n",
       "      <td>CstEf6M4JSom9Msm0qIYew</td>\n",
       "      <td>wdOOK3K6vzQy1d_OIk-U9w</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822484</th>\n",
       "      <td>Cwkkowhq9MZue1Xyk57BMg</td>\n",
       "      <td>7sPNbCx7vGAaH7SbNPZ6oA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id             business_id  stars\n",
       "3528833  U4INQZOPSUaj8hMjLlZ3KA  4JNXUYY8wbaaDmk3BPzlWw      3\n",
       "1966887  cg2P244yON3-_GXWkgAgsw  umXvdus9LbC6oxtLdXelFQ      4\n",
       "1777578  YHdXkAmndIfuIczWOnsjeQ  7HIa2lYy5jgcZuADlRjKSg      1\n",
       "1740381  CstEf6M4JSom9Msm0qIYew  wdOOK3K6vzQy1d_OIk-U9w      3\n",
       "1822484  Cwkkowhq9MZue1Xyk57BMg  7sPNbCx7vGAaH7SbNPZ6oA      4"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_train[['user_id','business_id','stars']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U4INQZOPSUaj8hMjLlZ3KA', '4JNXUYY8wbaaDmk3BPzlWw', 3.0, None),\n",
       " ('cg2P244yON3-_GXWkgAgsw', 'umXvdus9LbC6oxtLdXelFQ', 4.0, None),\n",
       " ('YHdXkAmndIfuIczWOnsjeQ', '7HIa2lYy5jgcZuADlRjKSg', 1.0, None),\n",
       " ('CstEf6M4JSom9Msm0qIYew', 'wdOOK3K6vzQy1d_OIk-U9w', 3.0, None),\n",
       " ('Cwkkowhq9MZue1Xyk57BMg', '7sPNbCx7vGAaH7SbNPZ6oA', 4.0, None)]"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw_ratings[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1867171</th>\n",
       "      <td>rdz9QHEPoecQ7DJk49ZI-g</td>\n",
       "      <td>ZrXYLXcEcDvYYBErmafobg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036395</th>\n",
       "      <td>uByIGcI5EJeKNGyBtCkWBw</td>\n",
       "      <td>HyKxWC9PrqlODjO-CBCIZA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119645</th>\n",
       "      <td>jmyunODJvYT7n7LCgotAyQ</td>\n",
       "      <td>0s1DqJlSSgR1Ftg9Q_HEOg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042659</th>\n",
       "      <td>HJj82f-csBI7jjgenwqhvw</td>\n",
       "      <td>ALp--wAIVL4qQfCANn3ffw</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920469</th>\n",
       "      <td>1dWLN4Mr4hKhu8MQUCKqXQ</td>\n",
       "      <td>WXSsJIO_uGGSxS9qC8x1gQ</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id             business_id  stars\n",
       "1867171  rdz9QHEPoecQ7DJk49ZI-g  ZrXYLXcEcDvYYBErmafobg      4\n",
       "1036395  uByIGcI5EJeKNGyBtCkWBw  HyKxWC9PrqlODjO-CBCIZA      4\n",
       "2119645  jmyunODJvYT7n7LCgotAyQ  0s1DqJlSSgR1Ftg9Q_HEOg      4\n",
       "1042659  HJj82f-csBI7jjgenwqhvw  ALp--wAIVL4qQfCANn3ffw      4\n",
       "3920469  1dWLN4Mr4hKhu8MQUCKqXQ  WXSsJIO_uGGSxS9qC8x1gQ      4"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_test[['user_id','business_id','stars']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rdz9QHEPoecQ7DJk49ZI-g', 'ZrXYLXcEcDvYYBErmafobg', 4.0, None),\n",
       " ('uByIGcI5EJeKNGyBtCkWBw', 'HyKxWC9PrqlODjO-CBCIZA', 4.0, None),\n",
       " ('jmyunODJvYT7n7LCgotAyQ', '0s1DqJlSSgR1Ftg9Q_HEOg', 4.0, None),\n",
       " ('HJj82f-csBI7jjgenwqhvw', 'ALp--wAIVL4qQfCANn3ffw', 4.0, None),\n",
       " ('1dWLN4Mr4hKhu8MQUCKqXQ', 'WXSsJIO_uGGSxS9qC8x1gQ', 4.0, None)]"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_ratings[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check more thoroughly with all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444292"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([444292, 444292])"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if train and test sets are splitted as before\n",
    "sum(df_sample_train[['user_id','business_id']].values == \n",
    "    np.array([list((user_id, business_id)) for user_id, business_id,_,_ in train_raw_ratings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444292"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_sample_train[['stars']].values.reshape((-1,len(df_sample_train)))[0] == \n",
    "    np.array([int(stars) for _, _,stars,_ in train_raw_ratings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49366"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49366, 49366])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if train and test sets are splitted as before\n",
    "sum(df_sample_test[['user_id','business_id']].values == \n",
    "    np.array([list((user_id, business_id)) for user_id, business_id,_,_ in test_raw_ratings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49366"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_sample_test[['stars']].values.reshape((-1,len(df_sample_test)))[0] == \n",
    "    np.array([int(stars) for _, _,stars,_ in test_raw_ratings]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers all match meaning both traing and test sets are same as before!\n",
    "\n",
    "Now I will choose the features of businesses to be used in machine learning. The target column is 'stars' for star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['stars']\n",
    "feature_col = list(df_sample.columns[2:-1]) # features of buiness to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latitude', 'longitude', 'price_range', 'business_review_count', 'Restaurants', 'Food', 'Nightlife', 'American (Traditional)', 'Bars', 'Fast Food', 'Mexican', 'American (New)', 'Sandwiches', 'Pizza', 'Coffee & Tea', 'Breakfast & Brunch', 'Burgers', 'Chinese', 'Italian', 'Desserts', 'Seafood', 'Japanese', 'Specialty Food', 'Sushi Bars', 'Asian Fusion', 'Bakeries', 'Salad', 'Shopping', 'Ice Cream & Frozen Yogurt', 'Juice Bars & Smoothies', 'Steakhouses', 'Chicken Wings', 'Cafes', 'Sports Bars', 'Grocery', 'Event Planning & Services', 'Barbeque', 'Beer', 'Wine & Spirits', 'Delis', 'Arts & Entertainment', 'Thai', 'Pubs', 'Mediterranean', 'Hawaiian', 'Diners', 'Caterers', 'Buffets', 'Drugstores', 'Vegetarian', 'Vegan', 'Cocktail Bars', 'Convenience Stores', 'Korean', 'Lounges', 'Bubble Tea', 'Soup', 'Wine Bars', 'Gluten-Free', 'Vietnamese', 'Food Delivery Services', 'Casinos', 'Noodles', 'Chicken Shop', 'Donuts', 'Hot Dogs', 'Tex-Mex', 'Ethnic Food', 'Food Trucks', 'Latin American', 'Filipino', 'French', 'Hotels & Travel', 'Fashion', 'Greek', 'Southern', 'Bagels', 'Middle Eastern', 'Tapas/Small Plates', 'Beauty & Spas', 'Department Stores', 'Health Markets', 'Indian', 'Tapas Bars', 'Shaved Ice', 'Comfort Food', 'Gastropubs', 'Breweries', 'Ramen', 'Automotive', 'Tacos', 'Chocolatiers & Shops', 'Venues & Event Spaces', 'Cupcakes', 'Hotels', 'Cosmetics & Beauty Supply', 'Candy Stores', 'Poke', 'Cajun/Creole', 'Beer Bar', 'Flowers & Gifts', 'Karaoke', 'Custom Cakes', 'Music Venues', 'Boulder City', 'Henderson', 'Las Vegas', 'North Las Vegas', 'Paradise']\n"
     ]
    }
   ],
   "source": [
    "print(feature_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_each_user(df_train, df_test, user_id):\n",
    "    X_train = df_train[df_train.user_id==u_id][feature_col].values\n",
    "    X_test = df_test[df_test.user_id==u_id][feature_col].values\n",
    "    y_train = df_train[df_train.user_id==u_id][target_col].values\n",
    "    y_test = df_test[df_test.user_id==u_id][target_col].values\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fuux76suMAhWIpwQdNqIWg    2\n",
       "S0I7sHnJ820Bn6wkwETyxQ    3\n",
       "1Vte47rrbB8p15pmSBsbSw    3\n",
       "jBpjA8IBQYTDLrI2Mvdk-g    4\n",
       "rJSVEZkitmgEt0rPQEuzWQ    4\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_train.user_id.value_counts(ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0iM8uF2D9iL8eau_tkPlRA    1\n",
       "QSwpHbPjDhWYKP8qlmlkXg    1\n",
       "4oJeSkScjPxuDC_LeK3KiQ    1\n",
       "1FkUTet5GQ5mVwNLw_Ds4g    1\n",
       "JReis6Ppj2VjKxJMZKt4Pw    1\n",
       "Name: business_id, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_train.business_id.value_counts(ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will build content-based filtering models using the basic concept of content-based filtering. My goal is still predicting star ratings with small RMSE. To predict stars, I use the business features (feature_col above) as predictors. Since every user has a different taste, parameters are fitted for each user. This is not collaborative filtering, so other users' ratings are not used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codf_sample_test.user_idusinesses in training for each user of test set\n",
    "business_count_each_users =[]\n",
    "for u_id in df_sample_test.user_id.unique():\n",
    "    count = df_sample_train[df_sample_train.user_id == u_id].business_id.nunique()\n",
    "    business_count_each_users.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_count_each_users = pd.Series(business_count_each_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "business_count_each_users.hist(bins=50)\n",
    "plt.xlabel('Review counts of each user in training')\n",
    "plt.ylabel('Number of users')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows sizes of training sets each for each user parameter training. The distribution is very right-skewed (i.e., a lot of users with a small number of reviews). There is one user who had almost 1400 business reviews (in the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "business_count_each_users[business_count_each_users<10].hist()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2829"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(business_count_each_users<10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for each user is feasible since many users (almost 3000) have not even 10 business reviews in the training set, so I will manually tune hyper-parameters when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_content_based_1(df_train, df_test, regr, counts=True):\n",
    "    '''\n",
    "    Function that trains and tests linear models and outputs results.\n",
    "    '''    \n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    mu = df_train.stars.mean()  \n",
    "    \n",
    "    for u_id in df_test.user_id.unique():\n",
    "            \n",
    "            X_train = df_train[df_train.user_id==u_id][feature_col].values\n",
    "            X_test = df_test[df_test.user_id==u_id][feature_col].values\n",
    "            y_train = df_train[df_train.user_id==u_id][target_col].values\n",
    "            y_test = df_test[df_test.user_id==u_id][target_col].values\n",
    "            #X_train, X_test, y_train, y_test = get_X_y_each_user(df_train, df_test, u_id)\n",
    "            \n",
    "            if len(y_train)<2: #cannot train for this user\n",
    "                y_pred = mu # mean rating of training set\n",
    "                if len(y_train) == 1:\n",
    "                    SSE_train += (y_train.item(0) - mu)**2\n",
    "                    count_users_1_in_train +=1\n",
    "                else:\n",
    "                    count_users_missing +=1\n",
    "\n",
    "            else:\n",
    "                clf = regr\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_train = np.reshape(y_train,(-1,1)) ## -1 for unspecified value \n",
    "                #print(clf.predict(X_train), y_train)\n",
    "                #print(sum((clf.predict(X_train) - y_train)**2))\n",
    "                SSE_train += sum((clf.predict(X_train) - y_train)**2)\n",
    "            \n",
    "            count_train += len(y_train)          \n",
    "            y_test = np.reshape(y_test,(-1,1)) ## -1 for unspecified value \n",
    "            #print(y_pred, y_test)\n",
    "            #print(sum((y_pred - y_test)**2))\n",
    "            SSE_test += sum((y_pred - y_test)**2) # Add sum of SE for each user\n",
    "            count_test += len(y_test)\n",
    "    \n",
    "    if counts:\n",
    "        print('Number of users in test set:', df_test.user_id.nunique())\n",
    "        print('Number of users missing in the training set:', count_users_missing)\n",
    "        print('Number of users with one business in the training set:', count_users_1_in_train)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    #print(SSE_train)\n",
    "    #print(MSE_train)\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    #print(RMSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test set: 16388\n",
      "Number of users missing in the training set: 0\n",
      "Number of users with one business in the training set: 0\n",
      "\n",
      "RMSE on training set: 0.3704\n",
      "\n",
      "RMSE on test set: 50930839612556.086\n",
      "Wall time: 28min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = LinearRegression(normalize=True, n_jobs=-1)\n",
    "training_test_algorithm_content_based_1(df_sample_train, df_sample_test, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there were no users with zero or one reviews in the training set, so the training set for every user was succefully trained (note that my training and testing function was prepared to use the average stars for those cases). \n",
    "\n",
    "This is the most simplest linear model and the overfitting is tragic. It is possibly due to small numbers of reviews for some users and large number of features. I definitely need regularizaion for my linear model and I chose Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test set: 16388\n",
      "Number of users missing in the training set: 0\n",
      "Number of users with one business in the training set: 0\n",
      "\n",
      "RMSE on training set: 0.6465\n",
      "\n",
      "RMSE on test set: 2.216\n",
      "Wall time: 30min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = Ridge(normalize=True, random_state=32, solver='auto') #alpha=1.0 by default\n",
    "training_test_algorithm_content_based_1(df_sample_train, df_sample_test, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge is so much better than linear regression without regularization!! I will increase the strength of regularization to see if it can further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test set: 16388\n",
      "Number of users missing in the training set: 0\n",
      "Number of users with one business in the training set: 0\n",
      "\n",
      "RMSE on training set: 0.7477\n",
      "\n",
      "RMSE on test set: 1.5555\n",
      "Wall time: 32min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = Ridge(normalize=True, random_state=32, solver='auto', alpha=2) \n",
    "training_test_algorithm_content_based_1(df_sample_train, df_sample_test, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, increasing regularization (from alpha=1 to alpha=2) improved the performance a lot!! RMSE was reduced from 2.216 to 1.5555. I can further improve this model by tuning alpha, but now I will try more complex models first and see if tuning them is more worth. \n",
    "\n",
    "I will try tree-based ensemble models, Random Forest and LightGBM. I found the above linear models and tree-based models work differently on numpy arrays, so I slightly modified my function for training and testing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_content_based_2(df_train, df_test, regr, counts=True):\n",
    "    '''\n",
    "    Function that trains and tests tree-based models and outputs results.\n",
    "    '''     \n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    mu = df_train.stars.mean()  \n",
    "    \n",
    "    for u_id in df_test.user_id.unique():\n",
    "            \n",
    "            X_train = df_train[df_train.user_id==u_id][feature_col].values\n",
    "            X_test = df_test[df_test.user_id==u_id][feature_col].values\n",
    "            y_train = df_train[df_train.user_id==u_id][target_col].values\n",
    "            y_test = df_test[df_test.user_id==u_id][target_col].values\n",
    "            #X_train, X_test, y_train, y_test = get_X_y_each_user(df_train, df_test, u_id)\n",
    "            \n",
    "            if len(y_train)<2: #cannot train for the user\n",
    "                y_pred = mu # mean rating of training set\n",
    "                if len(y_train) == 1:\n",
    "                    SSE_train += (y_train - mu)**2\n",
    "                    count_users_1_in_train +=1\n",
    "                else:\n",
    "                    count_users_missing +=1\n",
    "\n",
    "            else:\n",
    "                clf = regr\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "                #print(clf.predict(X_train), y_train)\n",
    "                #print(sum((clf.predict(X_train) - y_train)**2))\n",
    "                SSE_train += sum((clf.predict(X_train) - y_train)**2)\n",
    "            \n",
    "            count_train += len(y_train)          \n",
    "            y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "            #print(y_pred, y_test)\n",
    "            #print(sum((y_pred - y_test)**2))\n",
    "            SSE_test += sum((y_pred - y_test)**2) # Add sum of SE for each user\n",
    "            count_test += len(y_test)\n",
    "    \n",
    "    if counts:\n",
    "        print('Number of users in test set:', df_test.user_id.nunique())\n",
    "        print('Number of users missing in the training set:', count_users_missing)\n",
    "        print('Number of users with one business in the training set:', count_users_1_in_train)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    #print(SSE_train)\n",
    "    #print(MSE_train)\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    #print(RMSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test set: 16388\n",
      "Number of users missing in the training set: 0\n",
      "Number of users with one business in the training set: 0\n",
      "\n",
      "RMSE on training set: 0.503\n",
      "\n",
      "RMSE on test set: 1.2889\n",
      "Wall time: 2h 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "regr = RandomForestRegressor(random_state=32, n_jobs=-1)\n",
    "training_test_algorithm_content_based_2(df_sample_train, df_sample_test, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is much better than Ridge regression models (although it is slow)! I will reduce the number of features for each estimator (max_features) since I have too many features (109 of them) comparing to the training set sizes. Square root of the number of all features is a popular choice for max_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test set: 16388\n",
      "Number of users missing in the training set: 0\n",
      "Number of users with one business in the training set: 0\n",
      "\n",
      "RMSE on training set: 0.4938\n",
      "\n",
      "RMSE on test set: 1.2528\n",
      "Wall time: 2h 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "regr = RandomForestRegressor(random_state=32, n_jobs=-1, max_features='sqrt' )\n",
    "training_test_algorithm_content_based_2(df_sample_train, df_sample_test, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than using all features, but overfitting is still serious. I can definitely futher improve the performance by reducing overfitting (e.g. by setting maximum tree depth or maximum leaf nodes). However, this algorithm is so slow and Light GBM is known to be so much faster than Random Forest. Thus, I will move on to Light GBM and see if tuning LighrGBM is worth more than tuning RF.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0873\n",
      "\n",
      "RMSE on test set: 1.1869\n",
      "Wall time: 42min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, \n",
    "                        feature_fraction=.1, n_estimators=500) \n",
    "training_test_algorithm_content_based_2(df_sample_train, df_sample_test, regr, counts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LighGBM is so much faster than RF (42 min vs 2 hrs) and has much lower RMSE (1.1869 vs 1.2528)! Bagging won't be used for LightGBM due to those users with small training set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature fraction: 0.1\n",
      "\n",
      "RMSE on training set: 1.1056\n",
      "\n",
      "RMSE on test set: 1.1829\n",
      "\n",
      "Feature fraction: 0.2\n",
      "\n",
      "RMSE on training set: 1.1036\n",
      "\n",
      "RMSE on test set: 1.1825\n",
      "\n",
      "Feature fraction: 0.5\n",
      "\n",
      "RMSE on training set: 1.095\n",
      "\n",
      "RMSE on test set: 1.1817\n",
      "\n",
      "Feature fraction: 1\n",
      "\n",
      "RMSE on training set: 1.0901\n",
      "\n",
      "RMSE on test set: 1.1824\n",
      "\n",
      "Wall time: 2h 33min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fraction in [.1,.2,.5, 1]:\n",
    "    print(\"Feature fraction:\", fraction)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, \n",
    "                             feature_fraction=fraction) \n",
    "    training_test_algorithm_content_based_2(df_sample_train, df_sample_test, regr, counts=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is not a big issue here, but I will tune some parameters to see if I can further reduce variance. \n",
    "\n",
    "Meanwhile I realized that making training and test sets for each user takes almost 30 minutes, so I stored them in a dictionary with user ids for keys whose corresponding values are a list of 4 datasets, X_train, X_test, y_train, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dic_for_each_user(df_train, df_test, features):\n",
    "    '''\n",
    "    Makes user_id:[X_train,X_test,y_train,y_test] pair dictionary\n",
    "    '''\n",
    "    data_for_each_user = defaultdict(list) \n",
    "    for u_id in df_test.user_id.unique():\n",
    "        data_for_each_user[u_id].append(df_train[df_train.user_id==u_id][features].values)  \n",
    "        data_for_each_user[u_id].append(df_test[df_test.user_id==u_id][features].values)\n",
    "        data_for_each_user[u_id].append(df_train[df_train.user_id==u_id].stars.values)\n",
    "        data_for_each_user[u_id].append(df_test[df_test.user_id==u_id].stars.values)\n",
    "    \n",
    "    return data_for_each_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_y_train_test_all_features = make_data_dic_for_each_user(df_sample_train, df_sample_test, feature_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7634797293849935\n"
     ]
    }
   ],
   "source": [
    "# mean of y_train\n",
    "sum_stars=0\n",
    "count=0\n",
    "for datasets in X_y_train_test_all_features.values():\n",
    "    sum_stars += sum(datasets[2]) #y_train\n",
    "    count += len(datasets[2])\n",
    "mu = sum_stars/count\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_content_based_3(X_y_train_test_dict, regr, counts=False):\n",
    "    '''\n",
    "    Function that trains and tests tree-based models and outputs results.\n",
    "    The first argument X_y_train_test_dict is a premade dictionary with \n",
    "    user_id:[X_train,X_test,y_train,y_test] pairs for each user\n",
    "    '''     \n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    \n",
    "    # mean of y_train\n",
    "    sum_stars=0\n",
    "    count=0\n",
    "    for datasets in X_y_train_test_all_features.values():\n",
    "        sum_stars += sum(datasets[2]) #y_train\n",
    "        count += len(datasets[2])\n",
    "    mu = sum_stars/count\n",
    "    \n",
    "    for X_train, X_test, y_train, y_test in X_y_train_test_dict.values():\n",
    "            \n",
    "            if len(y_train)<2: #cannot train for the user\n",
    "                y_pred = mu # mean rating of training set\n",
    "                if len(y_train) == 1:\n",
    "                    SSE_train += (y_train - mu)**2\n",
    "                    count_users_1_in_train +=1\n",
    "                else:\n",
    "                    count_users_missing +=1\n",
    "\n",
    "            else:\n",
    "                clf = regr\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "                #print(clf.predict(X_train), y_train)\n",
    "                #print(sum((clf.predict(X_train) - y_train)**2))\n",
    "                SSE_train += sum((clf.predict(X_train) - y_train)**2)\n",
    "            \n",
    "            count_train += len(y_train)          \n",
    "            y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "            #print(y_pred, y_test)\n",
    "            #print(sum((y_pred - y_test)**2))\n",
    "            SSE_test += sum((y_pred - y_test)**2) # Add sum of SE for each user\n",
    "            count_test += len(y_test)\n",
    "    \n",
    "    if counts:\n",
    "        print('Number of users in test set:', df_test.user_id.nunique())\n",
    "        print('Number of users missing in the training set:', count_users_missing)\n",
    "        print('Number of users with one business in the training set:', count_users_1_in_train)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    #print(SSE_train)\n",
    "    #print(MSE_train)\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    #print(RMSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.095\n",
      "RMSE on test set: 1.1817\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_all_features, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is blazingly fast!!! Now I got the power for tuning!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1817\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5, \n",
    "                        max_depth=5, num_leaves=25) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_all_features, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the max tree depth and leaves (ealier they were free to grow) did not change the performance at all. This means the trees were smaller than the maximum numbers I set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth: 2    Number of leaves: 20   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1066\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 20   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.1054\n",
      "RMSE on test set: 1.1826\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 20   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 20   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0958\n",
      "RMSE on test set: 1.1822\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 30   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1066\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 30   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.1054\n",
      "RMSE on test set: 1.1826\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 30   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 30   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0958\n",
      "RMSE on test set: 1.1822\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 40   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1066\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 40   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.1054\n",
      "RMSE on test set: 1.1826\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 40   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 2    Number of leaves: 40   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0958\n",
      "RMSE on test set: 1.1822\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 20   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1061\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 20   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.1045\n",
      "RMSE on test set: 1.1825\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 20   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 20   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0932\n",
      "RMSE on test set: 1.1823\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 30   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1061\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 30   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.1045\n",
      "RMSE on test set: 1.1825\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 30   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 30   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0932\n",
      "RMSE on test set: 1.1823\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 40   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1061\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 40   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.1045\n",
      "RMSE on test set: 1.1825\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 40   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 3    Number of leaves: 40   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0932\n",
      "RMSE on test set: 1.1823\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 20   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1059\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 20   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.104\n",
      "RMSE on test set: 1.1825\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 20   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0962\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 20   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0918\n",
      "RMSE on test set: 1.1824\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 30   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1059\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 30   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.104\n",
      "RMSE on test set: 1.1825\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 30   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0962\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 30   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0918\n",
      "RMSE on test set: 1.1824\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 40   Number of features: 0.1\n",
      "\n",
      "RMSE on training set: 1.1059\n",
      "RMSE on test set: 1.1829\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 40   Number of features: 0.2\n",
      "\n",
      "RMSE on training set: 1.104\n",
      "RMSE on test set: 1.1825\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 40   Number of features: 0.5\n",
      "\n",
      "RMSE on training set: 1.0962\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Max depth: 4    Number of leaves: 40   Number of features: 1.0\n",
      "\n",
      "RMSE on training set: 1.0918\n",
      "RMSE on test set: 1.1824\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Wall time: 45min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for d in [2,3,4]: # tuning max_depth\n",
    "    for l in [20,30,40]: # tuning num_leaves\n",
    "        for f in [.1,.2,.5,1.]: # tuning feature_fraction\n",
    "            print(\"Max depth:\", d, \"   Number of leaves:\", l, \"  Number of features:\", f)\n",
    "            regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, \n",
    "                                     feature_fraction=f, max_depth=d, num_leaves=l) \n",
    "            training_test_algorithm_content_based_3(X_y_train_test_all_features, regr)\n",
    "            print('---------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most lowerest RMSE on the test set is still 1.1817 (in 7 hyper-parameter combinations), which is same as the RMSE of the model without restricted max depth and max number of leaves.\n",
    "\n",
    "This RMSE value is the best among the models of content-based filtering, but it is still worse than the most of collaborative filtering algorithms; it is only better than the Normal Predictor (random guess around the mean) and Just mean algorithms. This could be possibly due to those users with a few business reviews (recall that there were almost 3,000 users with less than 10 reviews). The number of features (109) are too many for these users and even with a few features, it is hard to train a model with a few samples of data. Thus, I am going to try two things:\n",
    "- Reduce the number of features\n",
    "- Use other predicitons (such as mean or collaborative filtering predictions) for the users with the number of reviews less than some threshold (say 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce the  number of features\n",
    "\n",
    "To investigate which features are important, I am going to check feature importances of the user with the most reviews (almost 1400) found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bLbSNkLggFnqwNNzzq-Ijw'"
      ]
     },
     "execution_count": 991,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the user with the most reviews in the training set\n",
    "top_user_id = df_sample_test.user_id.unique()[business_count_each_users==business_count_each_users.max()].item()\n",
    "top_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample_train[df_sample_train.user_id==top_user_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She or he left 1397 business reviews. I will train his/her data to get feature importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training_test_algorithm_content_based_1_user(df_train, df_test, regr, user_id):\n",
    "    print('user id:', user_id)\n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "\n",
    "    X_train = df_train[df_train.user_id==user_id][feature_col].values\n",
    "    X_test = df_test[df_test.user_id==user_id][feature_col].values\n",
    "    y_train = df_train[df_train.user_id==user_id][target_col].values\n",
    "    y_test = df_test[df_test.user_id==user_id][target_col].values\n",
    "    clf = regr \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "    SSE_train += sum((clf.predict(X_train) - y_train)**2)\n",
    "\n",
    "    count_train += len(y_train)          \n",
    "    y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "    SSE_test += sum((y_pred - y_test)**2) # Add sum of SE for each user\n",
    "    count_test += len(y_test)\n",
    "\n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    print(\"RMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "\n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))\n",
    "    \n",
    "    return clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id: bLbSNkLggFnqwNNzzq-Ijw\n",
      "RMSE on training set: 0.6564\n",
      "RMSE on test set: 0.7629\n",
      "Wall time: 273 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=1.) \n",
    "clf = training_test_algorithm_content_based_1_user(df_sample_train, df_sample_test, regr, top_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that draw a feature importance plot\n",
    "def plot_feature_importances(model_name, importances, feature_names, num_features=None, \n",
    "                             fig_size=None, ax=None):\n",
    "    features_importances = pd.DataFrame(sorted(zip(feature_names, importances), \n",
    "                                           key = lambda tup: tup[1], reverse=True),\n",
    "                                   columns=['features','importances'])\n",
    "    data = features_importances[:num_features]\n",
    "    data.plot('features','importances', kind='barh', ax=ax,\n",
    "              color='blue', figsize=fig_size, legend = None)\n",
    "    plt.gca().invert_yaxis() # reverse the order of features\n",
    "    plt.ylabel('feature importances')\n",
    "    if num_features:\n",
    "        plt.title(model_name + '\\nTop '+str(num_features))\n",
    "    else: \n",
    "        plt.title(model_name + '\\nAll Features')\n",
    "    \n",
    "    return features_importances # return all importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# top 10 features\n",
    "importances = plot_feature_importances('LightGBM', clf.feature_importances_, feature_col, \n",
    "                                       fig_size=(8,6), num_features=20)\n",
    "plt.tight_layout() # need for savefig\n",
    "plt.savefig(\"Feature_importances_LightGBM_20.png\",format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longitude</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latitude</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business_review_count</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price_range</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Desserts</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pizza</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bakeries</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mexican</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Juice Bars &amp; Smoothies</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Breakfast &amp; Brunch</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Italian</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>American (Traditional)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wine Bars</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>American (New)</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Noodles</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nightlife</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Restaurants</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sandwiches</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tapas/Small Plates</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ice Cream &amp; Frozen Yogurt</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Food Trucks</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Salad</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Gluten-Free</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Coffee &amp; Tea</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Event Planning &amp; Services</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Specialty Food</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Burgers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Steakhouses</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Grocery</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sushi Bars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Asian Fusion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     features  importances\n",
       "0                   longitude          860\n",
       "1                    latitude          840\n",
       "2       business_review_count          561\n",
       "3                 price_range          117\n",
       "4                        Food           75\n",
       "5                    Desserts           70\n",
       "6                       Pizza           47\n",
       "7                    Bakeries           46\n",
       "8                    Japanese           44\n",
       "9                     Mexican           41\n",
       "10     Juice Bars & Smoothies           29\n",
       "11                 Vegetarian           28\n",
       "12         Breakfast & Brunch           22\n",
       "13                    Italian           22\n",
       "14     American (Traditional)           20\n",
       "15                  Wine Bars           18\n",
       "16             American (New)           17\n",
       "17                   Shopping           17\n",
       "18                    Noodles           17\n",
       "19                  Nightlife           16\n",
       "20                Restaurants           12\n",
       "21                 Sandwiches           12\n",
       "22         Tapas/Small Plates           12\n",
       "23  Ice Cream & Frozen Yogurt           11\n",
       "24                Food Trucks            9\n",
       "25                      Salad            6\n",
       "26                Gluten-Free            6\n",
       "27               Coffee & Tea            5\n",
       "28  Event Planning & Services            4\n",
       "29             Specialty Food            3\n",
       "30              Mediterranean            3\n",
       "31                    Burgers            2\n",
       "32                Steakhouses            2\n",
       "33                    Grocery            2\n",
       "34                      Vegan            2\n",
       "35                 Sushi Bars            1\n",
       "36               Asian Fusion            1"
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances[importances['importances']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances for the top users show only top 37 features some importances in the LightGBM model. Thus, I will use these 37 features to train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_y_train_test_37_features = make_data_dic_for_each_user(df_sample_train, df_sample_test, list(importances[:37].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0903\n",
      "RMSE on test set: 1.1826\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=1.) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_37_features, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0947\n",
      "RMSE on test set: 1.1818\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_37_features, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only 37 features gives almost the same RMSE on the test set (only 0.0001 bigger than using all features). The hyper-parameter feature_fraction=1. seems to overfit even with the 37 features. I will further reduce the number of features and check if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_y_train_test_5_features = make_data_dic_for_each_user(df_sample_train, df_sample_test, list(importances[:5].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.091\n",
      "RMSE on test set: 1.1827\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=1.) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_5_features, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.096\n",
      "RMSE on test set: 1.1818\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_5_features, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dic_for_each_user_faster(df_train, df_test, features):\n",
    "    '''\n",
    "    Makes user_id:[X_train,X_test,y_train,y_test] pair dictionary\n",
    "    Twice faster than the previous version, but use more memory\n",
    "    '''\n",
    "    data_for_each_user = defaultdict(list) \n",
    "    \n",
    "    for u_id in df_test.user_id.unique():\n",
    "        train = df_train[df_train.user_id==u_id]\n",
    "        test =  df_test[df_test.user_id==u_id]\n",
    "        \n",
    "        data_for_each_user[u_id] = [train[features].values, test[features].values,\n",
    "                                  train.stars.values, test.stars.values]\n",
    "    \n",
    "    return data_for_each_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_y_train_test_10_features = make_data_dic_for_each_user_faster(df_sample_train, df_sample_test, list(importances[:10].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0909\n",
      "RMSE on test set: 1.1826\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=1.) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_10_features, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1818\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_3(X_y_train_test_10_features, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried 5 and 10 features and .5 (and .5 and 1. for feature_fraction), but they gave very similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_content_based_4(X_y_train_test_dict, regr, threshold=2, counts=False):\n",
    "    '''\n",
    "    Function that trains and tests tree-based models and outputs results.\n",
    "    The first argument X_y_train_test_dict is a premade dictionary with \n",
    "    user_id:[X_train,X_test,y_train,y_test] pairs for each user\n",
    "    \n",
    "    threshold:\n",
    "    Users with the number of reviews less than the threshold will be not trained.\n",
    "    Instead, the mean of stars in the training set will be their predictions.\n",
    "    '''     \n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    \n",
    "    # mean of y_train\n",
    "    sum_stars=0\n",
    "    count=0\n",
    "    for datasets in X_y_train_test_all_features.values():\n",
    "        sum_stars += sum(datasets[2]) #y_train\n",
    "        count += len(datasets[2])\n",
    "    mu = sum_stars/count\n",
    "    \n",
    "    for X_train, X_test, y_train, y_test in X_y_train_test_dict.values():\n",
    "            \n",
    "            if len(y_train) < threshold: # users hard to train\n",
    "                y_pred = mu # mean rating of training set\n",
    "                if len(y_train) == 1:\n",
    "                    SSE_train += (y_train - mu)**2\n",
    "                    count_users_1_in_train +=1\n",
    "                else:\n",
    "                    count_users_missing +=1\n",
    "\n",
    "            else:\n",
    "                clf = regr\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "                SSE_train += sum((clf.predict(X_train) - y_train)**2)\n",
    "            \n",
    "            count_train += len(y_train)          \n",
    "            y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "            SSE_test += sum((y_pred - y_test)**2) # Add sum of SE for each user\n",
    "            count_test += len(y_test)\n",
    "    \n",
    "    if counts:\n",
    "        print('Number of users in test set:', df_test.user_id.nunique())\n",
    "        print('Number of users missing in the training set:', count_users_missing)\n",
    "        print('Number of users with one business in the training set:', count_users_1_in_train)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1818\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_4(X_y_train_test_10_features, regr, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 5\n",
      "\n",
      "RMSE on training set: 1.095\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Threshold: 10\n",
      "\n",
      "RMSE on training set: 1.057\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Threshold: 20\n",
      "\n",
      "RMSE on training set: 0.8659\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Threshold: 50\n",
      "\n",
      "RMSE on training set: 0.5828\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Threshold: 100\n",
      "\n",
      "RMSE on training set: 0.3756\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num in [5,10,20,50,100]:\n",
    "    print(\"Threshold:\", num)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_content_based_4(X_y_train_test_all_features, regr, threshold=num)\n",
    "    print('---------------------------------------------------------------------\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 0.8655\n",
      "RMSE on test set: 1.2016\n"
     ]
    }
   ],
   "source": [
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_4(X_y_train_test_37_features, regr, threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 0.8666\n",
      "RMSE on test set: 1.2016\n"
     ]
    }
   ],
   "source": [
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_4(X_y_train_test_10_features, regr, threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 0.8672\n",
      "RMSE on test set: 1.2016\n"
     ]
    }
   ],
   "source": [
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_content_based_4(X_y_train_test_5_features, regr, threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 0.8711\n",
      "RMSE on test set: 1.2015\n"
     ]
    }
   ],
   "source": [
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5,\n",
    "                         num_leaves=10, max_depth=2) \n",
    "training_test_algorithm_content_based_4(X_y_train_test_all_features, regr, threshold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using means for the users with low numbers of reviews actually made the predictions worse. As the threshold gets larger, the LightGBM overfits more and more. Reducing the number of features did not change RMSE. Now it's time to tune those hyper-paramters that reduce variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained over: 5   Number of leaves: 10   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 10   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 10   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0961\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 10   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0957\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 20   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 20   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 20   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0961\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 20   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 30   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 30   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 30   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0961\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 30   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 40   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 40   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 40   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0961\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 40   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 50   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0991\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 50   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0971\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 50   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0961\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 5   Number of leaves: 50   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0956\n",
      "RMSE on test set: 1.1817\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 10   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0613\n",
      "RMSE on test set: 1.185\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 10   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0592\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 10   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0582\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 10   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0577\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 20   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0613\n",
      "RMSE on test set: 1.185\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 20   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0592\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 20   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0581\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 20   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0576\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 30   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0613\n",
      "RMSE on test set: 1.185\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 30   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0592\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 30   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0581\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 30   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0576\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 40   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0613\n",
      "RMSE on test set: 1.185\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 40   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0592\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 40   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0581\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 40   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0576\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 50   Max depth: 2\n",
      "\n",
      "RMSE on training set: 1.0613\n",
      "RMSE on test set: 1.185\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 50   Max depth: 3\n",
      "\n",
      "RMSE on training set: 1.0592\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 50   Max depth: 4\n",
      "\n",
      "RMSE on training set: 1.0581\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 10   Number of leaves: 50   Max depth: 5\n",
      "\n",
      "RMSE on training set: 1.0576\n",
      "RMSE on test set: 1.1851\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 10   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.8711\n",
      "RMSE on test set: 1.2015\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 10   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.8686\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 10   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.8674\n",
      "RMSE on test set: 1.2015\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 10   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.8668\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 20   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.8711\n",
      "RMSE on test set: 1.2015\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 20   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.8686\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 20   Max depth: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 0.8673\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 20   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.8667\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 30   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.8711\n",
      "RMSE on test set: 1.2015\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 30   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.8686\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 30   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.8673\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 30   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.8667\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 40   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.8711\n",
      "RMSE on test set: 1.2015\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 40   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.8686\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 40   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.8673\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 40   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.8667\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 50   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.8711\n",
      "RMSE on test set: 1.2015\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 50   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.8686\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 50   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.8673\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 20   Number of leaves: 50   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.8667\n",
      "RMSE on test set: 1.2016\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 10   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.5905\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 10   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.5868\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 10   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.5849\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 10   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.5841\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 20   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.5905\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 20   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.5868\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 20   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.5849\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 20   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.5839\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 30   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.5905\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 30   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.5868\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 30   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.5849\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 30   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.5839\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 40   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.5905\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 40   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.5868\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 40   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.5849\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 40   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.5839\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 50   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.5905\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 50   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.5868\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 50   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.5849\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 50   Number of leaves: 50   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.5839\n",
      "RMSE on test set: 1.2211\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 10   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.3872\n",
      "RMSE on test set: 1.2292\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 10   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.3818\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 10   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.379\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 10   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.3777\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 20   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.3872\n",
      "RMSE on test set: 1.2292\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 20   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.3818\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 20   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.3789\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 20   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.3774\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 30   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.3872\n",
      "RMSE on test set: 1.2292\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 30   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.3818\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 30   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.3789\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 30   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.3774\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 40   Max depth: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 0.3872\n",
      "RMSE on test set: 1.2292\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 40   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.3818\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 40   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.3789\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 40   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.3774\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 50   Max depth: 2\n",
      "\n",
      "RMSE on training set: 0.3872\n",
      "RMSE on test set: 1.2292\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 50   Max depth: 3\n",
      "\n",
      "RMSE on training set: 0.3818\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 50   Max depth: 4\n",
      "\n",
      "RMSE on training set: 0.3789\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Trained over: 100   Number of leaves: 50   Max depth: 5\n",
      "\n",
      "RMSE on training set: 0.3774\n",
      "RMSE on test set: 1.2293\n",
      "---------------------------------------------------------------------\n",
      "Wall time: 1h 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for t in [5,10,20,50,100]: # \n",
    "    for l in [10,20,30,40,50]: # tuning num_leaves\n",
    "        for d in [2,3,4,5]: # tuning tuning max_depth\n",
    "            print(\"Trained over:\", t, \"  Number of leaves:\", l, \"  Max depth:\", d)\n",
    "            regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, \n",
    "                                     feature_fraction=.5, max_depth=d, num_leaves=l) \n",
    "            training_test_algorithm_content_based_4(X_y_train_test_all_features, regr, threshold=t)\n",
    "            print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the hyper-parameters did not improve the performance. The best RMSE for content-based filtering is still 1.1817. It looks like LGBM is doing its best and there is no room to improve using LGBM and these business features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-aware Collaborative Filtering (In progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Collaborative_filtering#Context-aware_collaborative_filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-aware Collaborative Filtering can use context to get a vector for each user or item and use the ratings similar (or all) users or items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_hybrid(X_y_train_test_dict, regr,\n",
    "                                   train_set, test_set, algorithm, param_grid, n_cv):\n",
    "    '''\n",
    "    - Context-aware Collaborative Filtering\n",
    "    - Predictions are average between BaselineOnly (collaborative) and LGBM (content-based)\n",
    "    '''     \n",
    "    \n",
    "    ##### Collaborative filtering using Surprise #####\n",
    "    data.raw_ratings = train_set\n",
    "\n",
    "    # grid search cross validation\n",
    "    gs = GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=n_cv, n_jobs=-1)\n",
    "    gs.fit(data)\n",
    "    best_algo = gs.best_estimator['rmse']\n",
    "\n",
    "    # Compute performance on the whole training set \n",
    "    trainset = data.build_full_trainset()\n",
    "    best_algo.fit(trainset)\n",
    "    pred_train = best_algo.test(trainset.build_testset())\n",
    "    \n",
    "    # Compute performance on test set\n",
    "    testset = data.construct_testset(test_set)  # testset is now the set B\n",
    "    pred_test = best_algo.test(testset)\n",
    "    \n",
    "    \n",
    "    ##### Ensemble with content-based filtering using Sklearn #####\n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    \n",
    "    for u_id, datasets in X_y_train_test_dict.items():\n",
    "            y_pred_train_coll = [result.est for result in pred_train if result.uid==u_id]\n",
    "            y_pred_test_coll = [result.est for result in pred_test if result.uid==u_id]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = datasets\n",
    "\n",
    "            clf = regr\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_train = (y_pred_train_coll + clf.predict(X_train))/2\n",
    "            y_pred_test = (y_pred_test_coll + clf.predict(X_test))/2\n",
    "            \n",
    "            y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "            SSE_train += sum((y_pred_train - y_train)**2)            \n",
    "            count_train += len(y_train)          \n",
    "            \n",
    "            y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "            SSE_test += sum((y_pred_test - y_test)**2) # Add sum of SE for each user\n",
    "            count_test += len(y_test)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "\n",
      "RMSE on training set: 1.0402\n",
      "RMSE on test set: 1.1127\n",
      "Wall time: 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "param_grid = {'bsl_options':{'method': ['sgd']}}\n",
    "training_test_algorithm_hybrid(X_y_train_test_all_features, regr,\n",
    "                               train_raw_ratings, test_raw_ratings, BaselineOnly, \n",
    "                               param_grid, n_cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- very slow due to collecting data for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "Test set  RMSE: 1.0891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0890715889329599"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   # Collaborative filtering using Surprise\n",
    "    data.raw_ratings = train_raw_ratings\n",
    "\n",
    "    # grid search cross validation\n",
    "    param_grid = {'bsl_options':{'method': ['sgd']}}\n",
    "    gs = GridSearchCV(BaselineOnly, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "    gs.fit(data)\n",
    "    best_algo = gs.best_estimator['rmse']\n",
    "\n",
    "    # Compute performance on the whole training set \n",
    "    trainset = data.build_full_trainset()\n",
    "    best_algo.fit(trainset)\n",
    "    pred_train = best_algo.test(trainset.build_testset())\n",
    "    \n",
    "    # Compute performance on test set\n",
    "    testset = data.construct_testset(test_raw_ratings)  # testset is now the set B\n",
    "    pred_test = best_algo.test(testset)\n",
    "    print('Test set', end='  ')\n",
    "    accuracy.rmse(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='rdz9QHEPoecQ7DJk49ZI-g', iid='ZrXYLXcEcDvYYBErmafobg', r_ui=4.0, est=4.39586816967608, details={'was_impossible': False}),\n",
       " Prediction(uid='uByIGcI5EJeKNGyBtCkWBw', iid='HyKxWC9PrqlODjO-CBCIZA', r_ui=4.0, est=2.701837582892849, details={'was_impossible': False}),\n",
       " Prediction(uid='jmyunODJvYT7n7LCgotAyQ', iid='0s1DqJlSSgR1Ftg9Q_HEOg', r_ui=4.0, est=3.2560633794744436, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZrXYLXcEcDvYYBErmafobg 4.39586816967608\n",
      "BjrKNWhtQkedHw8hP_0Bjg 4.208314859353514\n"
     ]
    }
   ],
   "source": [
    "for result in pred_test:\n",
    "    if result.uid=='rdz9QHEPoecQ7DJk49ZI-g':\n",
    "        print(result.iid, result.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.39586816967608, 4.208314859353514]"
      ]
     },
     "execution_count": 1130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[result.est for result in pred_test if result.uid=='rdz9QHEPoecQ7DJk49ZI-g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4], dtype=int64)"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_train_test_all_features['rdz9QHEPoecQ7DJk49ZI-g'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_predicitons_each_user(user_ids, train_set, test_set, algorithm, param_grid, n_cv=3):\n",
    "    ##### Collaborative filtering using Surprise #####\n",
    "    data.raw_ratings = train_set\n",
    "\n",
    "    # grid search cross validation\n",
    "    gs = GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=n_cv, n_jobs=-1)\n",
    "    gs.fit(data)\n",
    "    best_algo = gs.best_estimator['rmse']\n",
    "\n",
    "    # Compute performance on the whole training set \n",
    "    trainset = data.build_full_trainset()\n",
    "    best_algo.fit(trainset)\n",
    "    pred_train = best_algo.test(trainset.build_testset())\n",
    "    \n",
    "    # Compute performance on test set\n",
    "    testset = data.construct_testset(test_set)  # testset is now the set B\n",
    "    pred_test = best_algo.test(testset)\n",
    "    \n",
    "    # Save the result\n",
    "    pred_each_user_dic = defaultdict(list)\n",
    "    for u_id in user_ids:\n",
    "            y_pred_train_coll = [result.est for result in pred_train if result.uid==u_id]\n",
    "            y_pred_test_coll = [result.est for result in pred_test if result.uid==u_id]\n",
    "            pred_each_user_dic[u_id] = [y_pred_train_coll, y_pred_test_coll]\n",
    "    \n",
    "    return  pred_each_user_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "Wall time: 11min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'bsl_options':{'method': ['sgd']}}\n",
    "y_pred_train_test_coll = baseline_predicitons_each_user(df_sample_test.user_id.unique(),\n",
    "                                          train_raw_ratings, test_raw_ratings, BaselineOnly, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_hybrid_2(X_y_train_test_dict, regr, y_pred_train_test_coll, threshold=2):\n",
    "    '''\n",
    "    - Context-aware Collaborative Filtering\n",
    "    - Much faster from the original version because of precalculated BaselineOnly predictions (y_pred_train_test_coll)\n",
    "    - Predictions are average between BaselineOnly (collaborative) and LGBM (content-based)\n",
    "    - If the number of trainings for a user is less than threshold, use BaselineOnly \n",
    "    '''      \n",
    "    ##### Ensemble with content-based filtering using Sklearn #####\n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    \n",
    "    for u_id, datasets in X_y_train_test_dict.items():\n",
    "        \n",
    "        y_pred_train_coll, y_pred_test_coll = y_pred_train_test_coll[u_id]\n",
    "        X_train, X_test, y_train, y_test = datasets\n",
    "        \n",
    "        if len(y_train) < threshold: # users hard to train\n",
    "            y_pred_train = y_pred_train_coll\n",
    "            y_pred_test = y_pred_test_coll\n",
    "        else: \n",
    "            clf = regr\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_train = (y_pred_train_coll + clf.predict(X_train))/2\n",
    "            y_pred_test = (y_pred_test_coll + clf.predict(X_test))/2\n",
    "            \n",
    "        y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "        SSE_train += sum((y_pred_train - y_train)**2)            \n",
    "        count_train += len(y_train)          \n",
    "            \n",
    "        y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "        SSE_test += sum((y_pred_test - y_test)**2) # Add sum of SE for each user\n",
    "        count_test += len(y_test)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0402\n",
      "RMSE on test set: 1.1127\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_hybrid_2(X_y_train_test_all_features, regr, y_pred_train_test_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training set: 1.0396\n",
      "RMSE on test set: 1.1085\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "training_test_algorithm_hybrid_2(X_y_train_test_all_features, regr, y_pred_train_test_coll, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0396\n",
      "RMSE on test set: 1.1085\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0329\n",
      "RMSE on test set: 1.0982\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0249\n",
      "RMSE on test set: 1.0919\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0252\n",
      "RMSE on test set: 1.0898\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [10,20,50,100]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_2(X_y_train_test_all_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.0268\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 300\n",
      "\n",
      "RMSE on training set: 1.0278\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 400\n",
      "\n",
      "RMSE on training set: 1.0281\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0283\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n",
      "Baseline if less than 600\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 800\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 1000\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [200,300,400,500,600,800,1000]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_2(X_y_train_test_all_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_hybrid_3(X_y_train_test_dict, regr, y_pred_train_test_coll, threshold=2):\n",
    "    '''\n",
    "    - Context-aware Collaborative Filtering\n",
    "    - USe precalculated BaselineOnly predictions (y_pred_train_test_coll)\n",
    "    - Predictions are made by LGBM (content-based)\n",
    "    - If the number of trainings for a user is less than threshold, use BaselineOnly\n",
    "    '''      \n",
    "    ##### Ensemble with content-based filtering using Sklearn #####\n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    \n",
    "    for u_id, datasets in X_y_train_test_dict.items():\n",
    "        \n",
    "        y_pred_train_coll, y_pred_test_coll = y_pred_train_test_coll[u_id]\n",
    "        X_train, X_test, y_train, y_test = datasets\n",
    "        \n",
    "        if len(y_train) < threshold: # users hard to train\n",
    "            y_pred_train = y_pred_train_coll\n",
    "            y_pred_test = y_pred_test_coll\n",
    "        else: \n",
    "            clf = regr\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            \n",
    "        y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "        SSE_train += sum((y_pred_train - y_train)**2)            \n",
    "        count_train += len(y_train)          \n",
    "            \n",
    "        y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "        SSE_test += sum((y_pred_test - y_test)**2) # Add sum of SE for each user\n",
    "        count_test += len(y_test)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0905\n",
      "RMSE on test set: 1.1671\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0654\n",
      "RMSE on test set: 1.1324\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0361\n",
      "RMSE on test set: 1.1083\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0289\n",
      "RMSE on test set: 1.0974\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [10,20,50,100]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_3(X_y_train_test_all_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.0283\n",
      "RMSE on test set: 1.0921\n",
      "------------------------------------------------\n",
      "Baseline if less than 300\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0904\n",
      "------------------------------------------------\n",
      "Baseline if less than 400\n",
      "\n",
      "RMSE on training set: 1.0286\n",
      "RMSE on test set: 1.0902\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0897\n",
      "------------------------------------------------\n",
      "Baseline if less than 600\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0894\n",
      "------------------------------------------------\n",
      "Baseline if less than 800\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0893\n",
      "------------------------------------------------\n",
      "Baseline if less than 1000\n",
      "\n",
      "RMSE on training set: 1.0286\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [200,300,400,500,600,800,1000]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_3(X_y_train_test_all_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0865\n",
      "RMSE on test set: 1.1681\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0613\n",
      "RMSE on test set: 1.1334\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0326\n",
      "RMSE on test set: 1.1092\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0271\n",
      "RMSE on test set: 1.0979\n",
      "------------------------------------------------\n",
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.0276\n",
      "RMSE on test set: 1.0921\n",
      "------------------------------------------------\n",
      "Baseline if less than 300\n",
      "\n",
      "RMSE on training set: 1.0281\n",
      "RMSE on test set: 1.0905\n",
      "------------------------------------------------\n",
      "Baseline if less than 400\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0903\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0897\n",
      "------------------------------------------------\n",
      "Baseline if less than 600\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0894\n",
      "------------------------------------------------\n",
      "Baseline if less than 800\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0893\n",
      "------------------------------------------------\n",
      "Baseline if less than 1000\n",
      "\n",
      "RMSE on training set: 1.0286\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [10,20,50,100, 200,300,400,500,600,800,1000]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=1.) \n",
    "    training_test_algorithm_hybrid_3(X_y_train_test_5_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0915\n",
      "RMSE on test set: 1.1672\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0664\n",
      "RMSE on test set: 1.1324\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0374\n",
      "RMSE on test set: 1.1084\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0301\n",
      "RMSE on test set: 1.0975\n",
      "------------------------------------------------\n",
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.029\n",
      "RMSE on test set: 1.0923\n",
      "------------------------------------------------\n",
      "Baseline if less than 300\n",
      "\n",
      "RMSE on training set: 1.0287\n",
      "RMSE on test set: 1.0906\n",
      "------------------------------------------------\n",
      "Baseline if less than 400\n",
      "\n",
      "RMSE on training set: 1.0288\n",
      "RMSE on test set: 1.0903\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0286\n",
      "RMSE on test set: 1.0898\n",
      "------------------------------------------------\n",
      "Baseline if less than 600\n",
      "\n",
      "RMSE on training set: 1.0286\n",
      "RMSE on test set: 1.0895\n",
      "------------------------------------------------\n",
      "Baseline if less than 800\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0893\n",
      "------------------------------------------------\n",
      "Baseline if less than 1000\n",
      "\n",
      "RMSE on training set: 1.0286\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [10,20,50,100, 200,300,400,500,600,800,1000]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_3(X_y_train_test_5_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0402\n",
      "RMSE on test set: 1.1085\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0335\n",
      "RMSE on test set: 1.0982\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0255\n",
      "RMSE on test set: 1.092\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0257\n",
      "RMSE on test set: 1.0899\n",
      "------------------------------------------------\n",
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.0272\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n",
      "Baseline if less than 300\n",
      "\n",
      "RMSE on training set: 1.0279\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n",
      "Baseline if less than 400\n",
      "\n",
      "RMSE on training set: 1.0282\n",
      "RMSE on test set: 1.0893\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0283\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n",
      "Baseline if less than 600\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 800\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 1000\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [10,20,50,100, 200,300,400,500,600,800,1000]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_2(X_y_train_test_5_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0373\n",
      "RMSE on test set: 1.1086\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0305\n",
      "RMSE on test set: 1.0983\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0229\n",
      "RMSE on test set: 1.092\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0241\n",
      "RMSE on test set: 1.0898\n",
      "------------------------------------------------\n",
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.0264\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 300\n",
      "\n",
      "RMSE on training set: 1.0276\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 400\n",
      "\n",
      "RMSE on training set: 1.028\n",
      "RMSE on test set: 1.0892\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0282\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 600\n",
      "\n",
      "RMSE on training set: 1.0283\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 800\n",
      "\n",
      "RMSE on training set: 1.0284\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n",
      "Baseline if less than 1000\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0891\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [10,20,50,100, 200,300,400,500,600,800,1000]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=1.) \n",
    "    training_test_algorithm_hybrid_2(X_y_train_test_5_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_algorithm_hybrid_4(X_y_train_test_dict, regr, y_pred_train_test_coll, threshold=10):\n",
    "    '''\n",
    "    - Context-aware Collaborative Filtering\n",
    "    - USe precalculated BaselineOnly predictions (y_pred_train_test_coll)\n",
    "    - Predictions are made by LGBM (content-based) cross-validated (cv=5)\n",
    "    - If the number of trainings for a user is less than threshold, use BaselineOnly\n",
    "    - Due to cross validation with 5 folds, the threshold has to be at least 5.\n",
    "    '''      \n",
    "    ##### Ensemble with content-based filtering using Sklearn #####\n",
    "    SSE_train = 0\n",
    "    SSE_test = 0\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_users_missing = 0\n",
    "    count_users_1_in_train = 0\n",
    "    \n",
    "    for u_id, datasets in X_y_train_test_dict.items():\n",
    "        \n",
    "        y_pred_train_coll, y_pred_test_coll = y_pred_train_test_coll[u_id]\n",
    "        X_train, X_test, y_train, y_test = datasets\n",
    "        \n",
    "        if len(y_train) < threshold: # users hard to train\n",
    "            y_pred_train = y_pred_train_coll\n",
    "            y_pred_test = y_pred_test_coll\n",
    "        else: \n",
    "            param_grid_skl={}\n",
    "            cv = GridSearchCV_skl(regr,param_grid_skl, cv=5, scoring='neg_mean_squared_error') # scoring='neg_mean_squared_error'??\n",
    "            cv.fit(X_train, y_train)\n",
    "            y_pred_train = cv.predict(X_train)\n",
    "            y_pred_test = cv.predict(X_test)\n",
    "            \n",
    "        y_train = np.reshape(y_train,(1,-1))[0] ## -1 for unspecified value \n",
    "        SSE_train += sum((y_pred_train - y_train)**2)            \n",
    "        count_train += len(y_train)          \n",
    "            \n",
    "        y_test = np.reshape(y_test,(1,-1))[0] ## -1 for unspecified value \n",
    "        SSE_test += sum((y_pred_test - y_test)**2) # Add sum of SE for each user\n",
    "        count_test += len(y_test)\n",
    "    \n",
    "    MSE_train = SSE_train/count_train # mean SSE for all users\n",
    "    RMSE_train = np.sqrt(MSE_train)\n",
    "    print(\"\\nRMSE on training set:\", round(RMSE_train.item(0),4))\n",
    "    \n",
    "    MSE_test = SSE_test/count_test # mean SSE for all users\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    print(\"RMSE on test set:\", round(RMSE_test.item(0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline if less than 5\n",
      "\n",
      "RMSE on training set: 1.095\n",
      "RMSE on test set: 1.1817\n",
      "------------------------------------------------\n",
      "Baseline if less than 10\n",
      "\n",
      "RMSE on training set: 1.0905\n",
      "RMSE on test set: 1.1671\n",
      "------------------------------------------------\n",
      "Baseline if less than 20\n",
      "\n",
      "RMSE on training set: 1.0654\n",
      "RMSE on test set: 1.1324\n",
      "------------------------------------------------\n",
      "Baseline if less than 50\n",
      "\n",
      "RMSE on training set: 1.0361\n",
      "RMSE on test set: 1.1083\n",
      "------------------------------------------------\n",
      "Baseline if less than 100\n",
      "\n",
      "RMSE on training set: 1.0289\n",
      "RMSE on test set: 1.0974\n",
      "------------------------------------------------\n",
      "Baseline if less than 200\n",
      "\n",
      "RMSE on training set: 1.0283\n",
      "RMSE on test set: 1.0921\n",
      "------------------------------------------------\n",
      "Baseline if less than 500\n",
      "\n",
      "RMSE on training set: 1.0285\n",
      "RMSE on test set: 1.0897\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in [5,10,20,50,100,200,500]:\n",
    "    print(\"Baseline if less than\", t)\n",
    "    regr = lgb.LGBMRegressor(random_state=32, learning_rate=0.01, feature_fraction=.5) \n",
    "    training_test_algorithm_hybrid_4(X_y_train_test_all_features, regr, y_pred_train_test_coll, threshold=t)\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Surprise\n",
    "https://surprise.readthedocs.io/en/stable/index.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
